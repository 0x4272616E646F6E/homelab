apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp
  namespace: llm
spec:
  runtimeClassName: nvidia
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp
  template:
    metadata:
      labels:
        app: llama-cpp
    spec:
      initContainers:
        - name: model-downloader
          image: alpine:latest
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - |
            apk add --no-cache curl && \
            echo "Checking if model exists..." && \
            if [ ! -f /models/lucy-128k.gguf ]; then
              echo "Downloading model..."
              curl -L -o /models/lucy-128k.gguf "https://huggingface.co/Menlo/Lucy-128k-gguf/resolve/main/lucy_128k-Q4_K_M.gguf?download=true"
            else
              echo "Model already exists, skipping download."
            fi
          volumeMounts:
            - name: models
              mountPath: /models
      containers:
        - name: llama-cpp
          image: ghcr.io/ggml-org/llama.cpp:latest
          imagePullPolicy: Always
          args:
          - "-m"
          - "/models/lucy-128k.gguf"
          - "--port"
          - "8080"
          - "--host"
          - "0.0.0.0"
          - "--ctx-size"
          - "4096"
          - "--batch-size"
          - "512"
          - "--n-gpu-layers"
          - "99"
          - "--rope-scaling"
          - "yarn"
          - "--rope-scale"
          - "3.2"
          - "--yarn-orig-ctx"
          - "40960"
          - "--temp"
          - "0.7"
          - "--top-p"
          - "0.9"
          - "--top-k"
          - "20"
          - "--min-p"
          - "0.0"
          ports:
            - containerPort: 8080
              protocol: TCP
          env:
          - name: SYCL_DEVICE_FILTER
            value: "level_zero:gpu"
          - name: GGML_SYCL_DISABLE_GRAPH
            value: "1"
          - name: GGML_SYCL_FORCE_MMQ
            value: "1"
          - name: GGML_SYCL_DISABLE_OPT
            value: "0"
          - name: GGML_SYCL_DEBUG
            value: "0"
          - name: ZES_ENABLE_SYSMAN
            value: "1"
          volumeMounts:
            - name: models
              mountPath: /models
          resources:
            requests:
              cpu: "2"
              memory: "2Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
              nvidia.com/gpu: "1"
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-cpp-models