apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config-map
  namespace: kube-system
data:
  values.yaml: |-
      namespaceOverride: ""
      commonLabels: {}
      upgradeCompatibility: null
      debug:
        enabled: false
        verbose: ~
      rbac:
        create: true
      imagePullSecrets: []
      iptablesRandomFully: false
      kubeConfigPath: ""
      k8sServiceHost: "127.0.0.1"
      k8sServicePort: "6443"
      k8sServiceLookupConfigMapName: ""
      k8sServiceLookupNamespace: ""
      k8sClientRateLimit:
        qps: 50
        burst: 100
        operator:
          qps: 100
          burst: 200
      cluster:
        name: default
        id: 0
      serviceAccounts:
        cilium:
          create: true
          name: cilium
          automount: true
          annotations: {}
        nodeinit:
          create: true
          enabled: false
          name: cilium-nodeinit
          automount: true
          annotations: {}
        envoy:
          create: true
          name: cilium-envoy
          automount: true
          annotations: {}
        operator:
          create: true
          name: cilium-operator
          automount: true
          annotations: {}
        preflight:
          create: true
          name: cilium-pre-flight
          automount: true
          annotations: {}
        relay:
          create: true
          name: hubble-relay
          automount: true
          annotations: {}
        ui:
          create: true
          name: hubble-ui
          automount: true
          annotations: {}
        clustermeshApiserver:
          create: true
          name: clustermesh-apiserver
          automount: true
          annotations: {}
        # -- Clustermeshcertgen is used if clustermesh.apiserver.tls.auto.method=cronJob
        clustermeshcertgen:
          create: true
          name: clustermesh-apiserver-generate-certs
          automount: true
          annotations: {}
        # -- Hubblecertgen is used if hubble.tls.auto.method=cronJob
        hubblecertgen:
          create: true
          name: hubble-generate-certs
          automount: true
          annotations: {}
      terminationGracePeriodSeconds: 1
      agent: true
      name: cilium
      rollOutCiliumPods: false
      image:
        override: ~
        repository: "quay.io/cilium/cilium"
        tag: "v1.18.4"
        pullPolicy: "IfNotPresent"
        digest: ""
        useDigest: false
      scheduling:
        mode: kube-scheduler
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  k8s-app: cilium
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - operator: Exists
      priorityClassName: ""
      dnsPolicy: ""
      extraContainers: []
      extraInitContainers: []
      extraArgs: []
      extraEnv: []
      extraHostPathMounts: []
      extraVolumes: []
      extraVolumeMounts: []
      extraConfig: {}
      annotations: {}
      podSecurityContext:
        appArmorProfile:
          type: "Unconfined"
      podAnnotations: {}
      podLabels: {}
      resources: {}

      initResources: {}
      securityContext:
        privileged: false
        seLinuxOptions:
          level: 's0'
          type: 'spc_t'
        capabilities:
          ciliumAgent:
            - CHOWN
            - KILL
            - NET_ADMIN
            - NET_RAW
            - IPC_LOCK
            - SYS_RESOURCE
            # cri-o >= v1.22.0 or containerd >= v1.5.0.
            # If available, SYS_ADMIN can be removed.
            #- PERFMON
            #- BPF
            - DAC_OVERRIDE
            - FOWNER
            - SETGID
            - SETUID
          mountCgroup:
            - SYS_ADMIN
            # Used for nsenter
            - SYS_CHROOT
            - SYS_PTRACE
          applySysctlOverwrites:
            # Required in order to access host's /etc/sysctl.d dir
            - SYS_ADMIN
            # Used for nsenter
            - SYS_CHROOT
            - SYS_PTRACE
          cleanCiliumState:
            - NET_ADMIN
            # Used in iptables. Consider removing once we are iptables-free
            # - SYS_MODULE
            # We need it for now but might not need it for >= 5.11 specially
            # for the 'SYS_RESOURCE'.
            # In >= 5.8 there's already BPF and PERMON capabilities
            - SYS_ADMIN
            # Could be an alternative for the SYS_ADMIN for the RLIMIT_NPROC
            - SYS_RESOURCE
            # Both PERFMON and BPF requires kernel 5.8, container runtime
            # cri-o >= v1.22.0 or containerd >= v1.5.0.
            # If available, SYS_ADMIN can be removed.
            #- PERFMON
            #- BPF
      updateStrategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
      aksbyocni:
        enabled: false
      autoDirectNodeRoutes: true
      directRoutingSkipUnreachable: false
      annotateK8sNode: false
      azure:
        enabled: false
      alibabacloud:
        enabled: false
      bandwidthManager:
        enabled: false
        bbr: false
      nat46x64Gateway:
        enabled: false
      highScaleIPcache:
        enabled: false
      l2announcements:
        enabled: true
      l2podAnnouncements:
        enabled: true
        interface: "eth0"
      bgpControlPlane:
        enabled: false
        secretsNamespace:
          create: false
          name: kube-system
        statusReport:
          enabled: true
      pmtuDiscovery:
        enabled: false
      bpf:
        autoMount:
          enabled: true
        root: /sys/fs/bpf
        preallocateMaps: false
        authMapMax: ~
        ctAccounting: false
        ctTcpMax: ~
        ctAnyMax: ~
        distributedLRU:
          enabled: false
        events:
          default:
            rateLimit: ~
            burstLimit: ~
          drop:
            enabled: true
          policyVerdict:
            enabled: true
          trace:
            enabled: true
        lbMapMax: 65536
        natMax: ~
        neighMax: ~
        nodeMapMax: ~
        policyMapMax: 16384
        mapDynamicSizeRatio: ~
        monitorAggregation: medium
        monitorInterval: "5s"
        monitorFlags: "all"
        lbExternalClusterIP: false
        lbSourceRangeAllTypes: false
        lbAlgorithmAnnotation: false
        lbModeAnnotation: false
        masquerade: true
        hostLegacyRouting: ~
        tproxy: ~
        vlanBypass: [0]
        disableExternalIPMitigation: false
        enableTCX: true
        datapathMode: veth
      bpfClockProbe: false
      cleanBpfState: false
      cleanState: false
      waitForKubeProxy: false
      cni:
        install: true
        uninstall: false
        chainingMode: ~
        chainingTarget: ~
        exclusive: true
        logFile: /var/run/cilium/cilium-cni.log
        customConf: false
        confPath: /etc/cni/net.d
        binPath: /opt/cni/bin
        configMapKey: cni-config
        confFileMountPath: /tmp/cni-configuration
        hostConfDirMountPath: /host/etc/cni/net.d
        resources:
          requests:
            cpu: 100m
            memory: 10Mi
        enableRouteMTUForCNIChaining: false
      conntrackGCInterval: ""
      conntrackGCMaxInterval: ""
      crdWaitTimeout: ""
      customCalls:
        enabled: false
      daemon:
        runPath: "/var/run/cilium"
        configSources: ~
        allowedConfigOverrides: ~
        blockedConfigOverrides: ~
        enableSourceIPVerification: true
      devices: "eth0,wg0"
      enableRuntimeDeviceDetection: true
      forceDeviceDetection: false
      egressMasqueradeInterfaces: ""
      enableCiliumEndpointSlice: false
      ciliumEndpointSlice:
        enabled: false
        rateLimits:
          - nodes: 0
            limit: 10
            burst: 20
          - nodes: 100
            limit: 50
            burst: 100
      envoyConfig:
        enabled: false
        secretsNamespace:
          create: true
          name: cilium-secrets
        retryInterval: 15s
      ingressController:
        enabled: false
        default: false
        loadbalancerMode: dedicated
        enforceHttps: true
        enableProxyProtocol: false
        ingressLBAnnotationPrefixes: ['lbipam.cilium.io', 'nodeipam.cilium.io', 'service.beta.kubernetes.io', 'service.kubernetes.io', 'cloud.google.com']
        defaultSecretNamespace:
        defaultSecretName:
        secretsNamespace:
          create: true
          name: cilium-secrets
          sync: true
        service:
          name: cilium-ingress
          labels: {}
          annotations: {}
          type: LoadBalancer
          insecureNodePort: ~
          secureNodePort: ~
          loadBalancerClass: ~
          loadBalancerIP: ~
          allocateLoadBalancerNodePorts: ~
          externalTrafficPolicy: Cluster
        hostNetwork:
          enabled: false
          sharedListenerPort: 8080
          nodes:
            matchLabels: {}
      gatewayAPI:
        enabled: false
        enableProxyProtocol: false
        enableAppProtocol: false
        enableAlpn: false
        xffNumTrustedHops: 0
        externalTrafficPolicy: Cluster
        gatewayClass:
          create: auto
        secretsNamespace:
          create: true
          name: cilium-secrets
          sync: true
        hostNetwork:
          enabled: false
          nodes:
            matchLabels: {}
      enableXTSocketFallback: true
      encryption:
        enabled: true
        type: wireguard
        nodeEncryption: false
        strictMode:
          enabled: false
          cidr: ""
          allowRemoteNodeIdentities: false
        ipsec:
          keyFile: keys
          mountPath: /etc/ipsec
          secretName: cilium-ipsec-keys
          interface: ""
          keyWatcher: true
          keyRotationDuration: "5m"
          encryptedOverlay: false
        wireguard:
          persistentKeepalive: 0s
      endpointHealthChecking:
        enabled: true
      endpointRoutes:
        enabled: true
      k8sNetworkPolicy:
        enabled: true
      endpointLockdownOnMapOverflow: false
      eni:
        enabled: false
        updateEC2AdapterLimitViaAPI: true
        awsReleaseExcessIPs: false
        awsEnablePrefixDelegation: false
        ec2APIEndpoint: ""
        eniTags: {}
        gcInterval: ""
        gcTags: {}
        iamRole: ""
        subnetIDsFilter: []
        subnetTagsFilter: []
        instanceTagsFilter: []
      externalIPs:
        enabled: true
      gke:
        enabled: false
      healthChecking: true
      healthPort: 9879
      healthCheckICMPFailureThreshold: 3
      hostFirewall:
        enabled: false
      hostPort:
        enabled: false
      socketLB:
        enabled: false
      certgen:
        generateCA: true
        image:
          override: ~
          repository: "quay.io/cilium/certgen"
          tag: "v0.2.1"
          digest: "sha256:ab6b1928e9c5f424f6b0f51c68065b9fd85e2f8d3e5f21fbd1a3cb27e6fb9321"
          useDigest: true
          pullPolicy: "IfNotPresent"
        ttlSecondsAfterFinished: 1800
        podLabels: {}
        annotations:
          job: {}
          cronJob: {}
        nodeSelector: {}
        priorityClassName: ""
        tolerations: []
        extraVolumes: []
        extraVolumeMounts: []
        affinity: {}
      hubble:
        enabled: true
        annotations: {}
        metrics:
          enabled: ~
          enableOpenMetrics: false
          port: 9965
          tls:
            enabled: false
            server:
              existingSecret: ""
              cert: ""
              key: ""
              extraDnsNames: []
              extraIpAddresses: []
              mtls:
                enabled: false
                useSecret: false
                name: ~
                key: ca.crt
          serviceAnnotations: {}
          serviceMonitor:
            enabled: false
            labels: {}
            annotations: {}
            jobLabel: ""
            interval: "10s"
            relabelings:
              - sourceLabels:
                  - __meta_kubernetes_pod_node_name
                targetLabel: node
                replacement: ${1}
            metricRelabelings: ~
            tlsConfig: {}
          dashboards:
            enabled: false
            label: grafana_dashboard
            namespace: ~
            labelValue: "1"
            annotations: {}
          dynamic:
            enabled: false
            config:
              configMapName: cilium-dynamic-metrics-config
              createConfigMap: true
              content:
                - name: all
                  contextOptions: []
                  includeFilters: []
                  excludeFilters: []
        socketPath: /var/run/cilium/hubble.sock
        redact:
          enabled: false
          http:
            urlQuery: false
            userInfo: true
            headers:
              allow: []
              deny: []
          kafka:
            apiKey: false
        listenAddress: ":4244"
        preferIpv6: false
        skipUnknownCGroupIDs: ~
        peerService:
          targetPort: 4244
          clusterDomain: cluster.local
        tls:
          enabled: true
          auto:
            enabled: true
            method: helm
            certValidityDuration: 365
            schedule: "0 0 1 */4 *"
            certManagerIssuerRef: {}
          server:
            existingSecret: ""
            cert: ""
            key: ""
            extraDnsNames: []
            extraIpAddresses: []
        relay:
          enabled: true
          rollOutPods: false
          image:
            override: ~
            repository: "quay.io/cilium/hubble-relay"
            tag: "v1.18.4"
            digest: ""
            useDigest: false
            pullPolicy: "IfNotPresent"
          resources: {}
          replicas: 1
          affinity:
            podAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - topologyKey: kubernetes.io/hostname
                  labelSelector:
                    matchLabels:
                      k8s-app: cilium
          topologySpreadConstraints: []
          nodeSelector:
            kubernetes.io/os: linux
          tolerations: []
          extraEnv: []
          annotations: {}
          podAnnotations: {}
          podLabels: {}
          podDisruptionBudget:
            enabled: false
            minAvailable: null
            maxUnavailable: 1
          priorityClassName: ""
          terminationGracePeriodSeconds: 1
          updateStrategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 1
          extraVolumes: []
          extraVolumeMounts: []
          podSecurityContext:
            fsGroup: 65532
          securityContext:
            runAsNonRoot: true
            runAsUser: 65532
            runAsGroup: 65532
            capabilities:
              drop:
                - ALL
          service:
            type: ClusterIP
            nodePort: 31234
          listenHost: ""
          listenPort: "4245"
          tls:
            client:
              existingSecret: ""
              cert: ""
              key: ""
            server:
              enabled: false
              mtls: false
              existingSecret: ""
              # -- base64 encoded PEM values for the Hubble relay server certificate (deprecated).
              # Use existingSecret instead.
              cert: ""
              # -- base64 encoded PEM values for the Hubble relay server key (deprecated).
              # Use existingSecret instead.
              key: ""
              # -- extra DNS names added to certificate when its auto gen
              extraDnsNames: []
              # -- extra IP addresses added to certificate when its auto gen
              extraIpAddresses: []
              # DNS name used by the backend to connect to the relay
              # This is a simple workaround as the relay certificates are currently hardcoded to
              # *.hubble-relay.cilium.io
              # See https://github.com/cilium/cilium/pull/28709#discussion_r1371792546
              # For GKE Dataplane V2 this should be set to relay.kube-system.svc.cluster.local
              relayName: "ui.hubble-relay.cilium.io"
          # @schema
          # type: [null, string]
          # @schema
          # -- Dial timeout to connect to the local hubble instance to receive peer information (e.g. "30s").
          #
          # This option has been deprecated and is a no-op.
          dialTimeout: ~
          # @schema
          # type: [null, string]
          # @schema
          # -- Backoff duration to retry connecting to the local hubble instance in case of failure (e.g. "30s").
          retryTimeout: ~
          # @schema
          # type: [null, integer]
          # @schema
          # -- (int) Max number of flows that can be buffered for sorting before being sent to the
          # client (per request) (e.g. 100).
          sortBufferLenMax: ~
          # @schema
          # type: [null, string]
          # @schema
          # -- When the per-request flows sort buffer is not full, a flow is drained every
          # time this timeout is reached (only affects requests in follow-mode) (e.g. "1s").
          sortBufferDrainTimeout: ~
          # -- Port to use for the k8s service backed by hubble-relay pods.
          # If not set, it is dynamically assigned to port 443 if TLS is enabled and to
          # port 80 if not.
          # servicePort: 80

          # -- Enable prometheus metrics for hubble-relay on the configured port at
          # /metrics
          prometheus:
            enabled: false
            port: 9966
            serviceMonitor:
              # -- Enable service monitors.
              # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
              enabled: false
              # -- Labels to add to ServiceMonitor hubble-relay
              labels: {}
              # -- Annotations to add to ServiceMonitor hubble-relay
              annotations: {}
              # -- Interval for scrape metrics.
              interval: "10s"
              # -- Specify the Kubernetes namespace where Prometheus expects to find
              # service monitors configured.
              # namespace: ""
              # @schema
              # type: [null, array]
              # @schema
              # -- Relabeling configs for the ServiceMonitor hubble-relay
              relabelings: ~
              # @schema
              # type: [null, array]
              # @schema
              # -- Metrics relabeling configs for the ServiceMonitor hubble-relay
              metricRelabelings: ~
          gops:
            # -- Enable gops for hubble-relay
            enabled: true
            # -- Configure gops listen port for hubble-relay
            port: 9893
          pprof:
            # -- Enable pprof for hubble-relay
            enabled: false
            # -- Configure pprof listen address for hubble-relay
            address: localhost
            # -- Configure pprof listen port for hubble-relay
            port: 6062
        ui:
          # -- Whether to enable the Hubble UI.
          enabled: true
          standalone:
            # -- When true, it will allow installing the Hubble UI only, without checking dependencies.
            # It is useful if a cluster already has cilium and Hubble relay installed and you just
            # want Hubble UI to be deployed.
            # When installed via helm, installing UI should be done via `helm upgrade` and when installed via the cilium cli, then `cilium hubble enable --ui`
            enabled: false
            tls:
              # -- When deploying Hubble UI in standalone, with tls enabled for Hubble relay, it is required
              # to provide a volume for mounting the client certificates.
              certsVolume: {}
              #   projected:
              #     defaultMode: 0400
              #     sources:
              #     - secret:
              #         name: hubble-ui-client-certs
              #         items:
              #         - key: tls.crt
              #           path: client.crt
              #         - key: tls.key
              #           path: client.key
              #         - key: ca.crt
              #           path: hubble-relay-ca.crt
          # -- Roll out Hubble-ui pods automatically when configmap is updated.
          rollOutPods: false
          tls:
            client:
              # -- Name of the Secret containing the client certificate and key for Hubble UI
              # If specified, cert and key are ignored.
              existingSecret: ""
              # -- base64 encoded PEM values for the Hubble UI client certificate (deprecated).
              # Use existingSecret instead.
              cert: ""
              # -- base64 encoded PEM values for the Hubble UI client key (deprecated).
              # Use existingSecret instead.
              key: ""
          backend:
            # -- Hubble-ui backend image.
            image:
              # @schema
              # type: [null, string]
              # @schema
              override: ~
              repository: "quay.io/cilium/hubble-ui-backend"
              tag: "v0.13.2"
              digest: "sha256:a034b7e98e6ea796ed26df8f4e71f83fc16465a19d166eff67a03b822c0bfa15"
              useDigest: true
              pullPolicy: "IfNotPresent"
            # -- Hubble-ui backend security context.
            securityContext: {}
            # -- Additional hubble-ui backend environment variables.
            extraEnv: []
            # -- Additional hubble-ui backend volumes.
            extraVolumes: []
            # -- Additional hubble-ui backend volumeMounts.
            extraVolumeMounts: []
            livenessProbe:
              # -- Enable liveness probe for Hubble-ui backend (requires Hubble-ui 0.12+)
              enabled: false
            readinessProbe:
              # -- Enable readiness probe for Hubble-ui backend (requires Hubble-ui 0.12+)
              enabled: false
            # -- Resource requests and limits for the 'backend' container of the 'hubble-ui' deployment.
            resources: {}
            #   limits:
            #     cpu: 1000m
            #     memory: 1024M
            #   requests:
            #     cpu: 100m
            #     memory: 64Mi
          frontend:
            # -- Hubble-ui frontend image.
            image:
              # @schema
              # type: [null, string]
              # @schema
              override: ~
              repository: "quay.io/cilium/hubble-ui"
              tag: "v0.13.2"
              digest: "sha256:9e37c1296b802830834cc87342a9182ccbb71ffebb711971e849221bd9d59392"
              useDigest: true
              pullPolicy: "IfNotPresent"
            # -- Hubble-ui frontend security context.
            securityContext: {}
            # -- Additional hubble-ui frontend environment variables.
            extraEnv: []
            # -- Additional hubble-ui frontend volumes.
            extraVolumes: []
            # -- Additional hubble-ui frontend volumeMounts.
            extraVolumeMounts: []
            # -- Resource requests and limits for the 'frontend' container of the 'hubble-ui' deployment.
            resources: {}
            #   limits:
            #     cpu: 1000m
            #     memory: 1024M
            #   requests:
            #     cpu: 100m
            #     memory: 64Mi
            server:
              # -- Controls server listener for ipv6
              ipv6:
                enabled: true
          # -- The number of replicas of Hubble UI to deploy.
          replicas: 1
          # -- Annotations to be added to all top-level hubble-ui objects (resources under templates/hubble-ui)
          annotations: {}
          # -- Additional labels to be added to 'hubble-ui' deployment object
          labels: {}
          # -- Annotations to be added to hubble-ui pods
          podAnnotations: {}
          # -- Labels to be added to hubble-ui pods
          podLabels: {}
          # PodDisruptionBudget settings
          podDisruptionBudget:
            # -- enable PodDisruptionBudget
            # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
            enabled: false
            # @schema
            # type: [null, integer, string]
            # @schema
            # -- Minimum number/percentage of pods that should remain scheduled.
            # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
            minAvailable: null
            # @schema
            # type: [null, integer, string]
            # @schema
            # -- Maximum number/percentage of pods that may be made unavailable
            maxUnavailable: 1
          # -- Affinity for hubble-ui
          affinity: {}
          # -- Pod topology spread constraints for hubble-ui
          topologySpreadConstraints: []
          #   - maxSkew: 1
          #     topologyKey: topology.kubernetes.io/zone
          #     whenUnsatisfiable: DoNotSchedule

          # -- Node labels for pod assignment
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
          nodeSelector:
            kubernetes.io/os: linux
          # -- Node tolerations for pod assignment on nodes with taints
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
          tolerations: []
          # -- The priority class to use for hubble-ui
          priorityClassName: ""
          # -- hubble-ui update strategy.
          updateStrategy:
            type: RollingUpdate
            rollingUpdate:
              # @schema
              # type: [integer, string]
              # @schema
              maxUnavailable: 1
          # -- Security context to be added to Hubble UI pods
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
            fsGroup: 1001
          # -- hubble-ui service configuration.
          service:
            # -- Annotations to be added for the Hubble UI service
            annotations: {}
            # --- The type of service used for Hubble UI access, either ClusterIP or NodePort.
            type: ClusterIP
            # --- The port to use when the service type is set to NodePort.
            nodePort: 31235
          # -- Defines base url prefix for all hubble-ui http requests.
          # It needs to be changed in case if ingress for hubble-ui is configured under some sub-path.
          # Trailing `/` is required for custom path, ex. `/service-map/`
          baseUrl: "/"
          # -- hubble-ui ingress configuration.
          ingress:
            enabled: false
            annotations: {}
            # kubernetes.io/ingress.class: nginx
            # kubernetes.io/tls-acme: "true"
            className: ""
            hosts:
              - chart-example.local
            labels: {}
            tls: []
            #  - secretName: chart-example-tls
            #    hosts:
            #      - chart-example.local
        # -- Hubble flows export.
        export:
          # --- Defines max file size of output file before it gets rotated.
          fileMaxSizeMb: 10
          # --- Defines max number of backup/rotated files.
          fileMaxBackups: 5
          # --- Static exporter configuration.
          # Static exporter is bound to agent lifecycle.
          static:
            enabled: false
            filePath: /var/run/cilium/hubble/events.log
            fieldMask: []
            # - time
            # - source
            # - destination
            # - verdict
            allowList: []
            # - '{"verdict":["DROPPED","ERROR"]}'
            denyList: []
            # - '{"source_pod":["kube-system/"]}'
            # - '{"destination_pod":["kube-system/"]}'
          # --- Dynamic exporters configuration.
          # Dynamic exporters may be reconfigured without a need of agent restarts.
          dynamic:
            enabled: false
            config:
              # ---- Name of configmap with configuration that may be altered to reconfigure exporters within a running agents.
              configMapName: cilium-flowlog-config
              # ---- True if helm installer should create config map.
              # Switch to false if you want to self maintain the file content.
              createConfigMap: true
              # ---- Exporters configuration in YAML format.
              content:
                - name: all
                  fieldMask: []
                  includeFilters: []
                  excludeFilters: []
                  filePath: "/var/run/cilium/hubble/events.log"
                  #   - name: "test002"
                  #     filePath: "/var/log/network/flow-log/pa/test002.log"
                  #     fieldMask: ["source.namespace", "source.pod_name", "destination.namespace", "destination.pod_name", "verdict"]
                  #     includeFilters:
                  #     - source_pod: ["default/"]
                  #       event_type:
                  #       - type: 1
                  #     - destination_pod: ["frontend/nginx-975996d4c-7hhgt"]
                  #     excludeFilters: []
                  #     end: "2023-10-09T23:59:59-07:00"
        # -- Emit v1.Events related to pods on detection of packet drops.
        #    This feature is alpha, please provide feedback at https://github.com/cilium/cilium/issues/33975.
        dropEventEmitter:
          enabled: false
          # --- Minimum time between emitting same events.
          interval: 2m
          # --- Drop reasons to emit events for.
          # ref: https://docs.cilium.io/en/stable/_api/v1/flow/README/#dropreason
          reasons:
            - auth_required
            - policy_denied
      # -- Method to use for identity allocation (`crd`, `kvstore` or `doublewrite-readkvstore` / `doublewrite-readcrd` for migrating between identity backends).
      identityAllocationMode: "crd"
      # -- (string) Time to wait before using new identity on endpoint identity change.
      # @default -- `"5s"`
      identityChangeGracePeriod: ""
      # -- Install Iptables rules to skip netfilter connection tracking on all pod
      # traffic. This option is only effective when Cilium is running in direct
      # routing and full KPR mode. Moreover, this option cannot be enabled when Cilium
      # is running in a managed Kubernetes environment or in a chained CNI setup.
      installNoConntrackIptablesRules: false
      ipam:
        # -- Configure IP Address Management mode.
        # ref: https://docs.cilium.io/en/stable/network/concepts/ipam/
        mode: "kubernetes"
        # -- Maximum rate at which the CiliumNode custom resource is updated.
        ciliumNodeUpdateRate: "15s"
        # -- Pre-allocation settings for IPAM in Multi-Pool mode
        multiPoolPreAllocation: ""
        # -- Install ingress/egress routes through uplink on host for Pods when working with delegated IPAM plugin.
        installUplinkRoutesForDelegatedIPAM: false
        operator:
          # @schema
          # type: [array, string]
          # @schema
          # -- IPv4 CIDR list range to delegate to individual nodes for IPAM.
          clusterPoolIPv4PodCIDRList: ["10.0.0.0/8"]
          # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.
          clusterPoolIPv4MaskSize: 24
          # @schema
          # type: [array, string]
          # @schema
          # -- IPv6 CIDR list range to delegate to individual nodes for IPAM.
          clusterPoolIPv6PodCIDRList: ["fd00::/104"]
          # -- IPv6 CIDR mask size to delegate to individual nodes for IPAM.
          clusterPoolIPv6MaskSize: 120
          # -- IP pools to auto-create in multi-pool IPAM mode.
          autoCreateCiliumPodIPPools: {}
          #   default:
          #     ipv4:
          #       cidrs:
          #         - 10.10.0.0/8
          #       maskSize: 24
          #   other:
          #     ipv6:
          #       cidrs:
          #         - fd00:100::/80
          #       maskSize: 96
          # @schema
          # type: [null, integer]
          # @schema
          # -- (int) The maximum burst size when rate limiting access to external APIs.
          # Also known as the token bucket capacity.
          # @default -- `20`
          externalAPILimitBurstSize: ~
          # @schema
          # type: [null, number]
          # @schema
          # -- (float) The maximum queries per second when rate limiting access to
          # external APIs. Also known as the bucket refill rate, which is used to
          # refill the bucket up to the burst size capacity.
          # @default -- `4.0`
          externalAPILimitQPS: ~
      # -- defaultLBServiceIPAM indicates the default LoadBalancer Service IPAM when
      # no LoadBalancer class is set. Applicable values: lbipam, nodeipam, none
      # @schema
      # type: [string]
      # @schema
      defaultLBServiceIPAM: lbipam
      nodeIPAM:
        # -- Configure Node IPAM
        # ref: https://docs.cilium.io/en/stable/network/node-ipam/
        enabled: false
      # @schema
      # type: [null, string]
      # @schema
      # -- The api-rate-limit option can be used to overwrite individual settings of the default configuration for rate limiting calls to the Cilium Agent API
      apiRateLimit: ~
      # -- Configure the eBPF-based ip-masq-agent
      ipMasqAgent:
        enabled: false
      # the config of nonMasqueradeCIDRs
      # config:
      #   nonMasqueradeCIDRs: []
      #   masqLinkLocal: false
      #   masqLinkLocalIPv6: false

      # iptablesLockTimeout defines the iptables "--wait" option when invoked from Cilium.
      # iptablesLockTimeout: "5s"
      ipv4:
        # -- Enable IPv4 support.
        enabled: true
      ipv6:
        # -- Enable IPv6 support.
        enabled: false
      # -- Configure Kubernetes specific configuration
      k8s:
        # -- requireIPv4PodCIDR enables waiting for Kubernetes to provide the PodCIDR
        # range via the Kubernetes node resource
        requireIPv4PodCIDR: false
        # -- requireIPv6PodCIDR enables waiting for Kubernetes to provide the PodCIDR
        # range via the Kubernetes node resource
        requireIPv6PodCIDR: false
      # -- Keep the deprecated selector labels when deploying Cilium DaemonSet.
      keepDeprecatedLabels: false
      # -- Keep the deprecated probes when deploying Cilium DaemonSet
      keepDeprecatedProbes: false
      startupProbe:
        # -- failure threshold of startup probe.
        # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
        failureThreshold: 105
        # -- interval between checks of the startup probe
        periodSeconds: 2
      livenessProbe:
        # -- failure threshold of liveness probe
        failureThreshold: 10
        # -- interval between checks of the liveness probe
        periodSeconds: 30
      readinessProbe:
        # -- failure threshold of readiness probe
        failureThreshold: 3
        # -- interval between checks of the readiness probe
        periodSeconds: 30
      # -- Configure the kube-proxy replacement in Cilium BPF datapath
      # Valid options are "true" or "false".
      # ref: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/
      kubeProxyReplacement: "true"

      # -- healthz server bind address for the kube-proxy replacement.
      # To enable set the value to '0.0.0.0:10256' for all ipv4
      # addresses and this '[::]:10256' for all ipv6 addresses.
      # By default it is disabled.
      kubeProxyReplacementHealthzBindAddr: ""
      l2NeighDiscovery:
        # -- Enable L2 neighbor discovery in the agent
        enabled: true
        # -- Override the agent's default neighbor resolution refresh period.
        refreshPeriod: "30s"
      # -- Enable Layer 7 network policy.
      l7Proxy: true
      # -- Enable Local Redirect Policy.
      localRedirectPolicy: false
      # To include or exclude matched resources from cilium identity evaluation
      # labels: ""

      # logOptions allows you to define logging options. eg:
      # logOptions:
      #   format: json

      # -- Enables periodic logging of system load
      logSystemLoad: false
      # -- Configure maglev consistent hashing
      maglev: {}
      # -- tableSize is the size (parameter M) for the backend table of one
      # service entry
      # tableSize:

      # -- hashSeed is the cluster-wide base64 encoded seed for the hashing
      # hashSeed:

      # -- Enables masquerading of IPv4 traffic leaving the node from endpoints.
      enableIPv4Masquerade: true
      # -- Enables masquerading of IPv6 traffic leaving the node from endpoints.
      enableIPv6Masquerade: true
      # -- Enables masquerading to the source of the route for traffic leaving the node from endpoints.
      enableMasqueradeRouteSource: false
      # -- Enables IPv4 BIG TCP support which increases maximum IPv4 GSO/GRO limits for nodes and pods
      enableIPv4BIGTCP: false
      # -- Enables IPv6 BIG TCP support which increases maximum IPv6 GSO/GRO limits for nodes and pods
      enableIPv6BIGTCP: false
      nat:
        # -- Number of the top-k SNAT map connections to track in Cilium statedb.
        mapStatsEntries: 32
        # -- Interval between how often SNAT map is counted for stats.
        mapStatsInterval: 30s
      egressGateway:
        # -- Enables egress gateway to redirect and SNAT the traffic that leaves the
        # cluster.
        enabled: true
        # -- Time between triggers of egress gateway state reconciliations
        reconciliationTriggerInterval: 1s
        # -- Maximum number of entries in egress gateway policy map
        # maxPolicyEntries: 16384
      vtep:
        # -- Enables VXLAN Tunnel Endpoint (VTEP) Integration (beta) to allow
        # Cilium-managed pods to talk to third party VTEP devices over Cilium tunnel.
        enabled: false
        # -- A space separated list of VTEP device endpoint IPs, for example "1.1.1.1  1.1.2.1"
        endpoint: ""
        # -- A space separated list of VTEP device CIDRs, for example "1.1.1.0/24 1.1.2.0/24"
        cidr: ""
        # -- VTEP CIDRs Mask that applies to all VTEP CIDRs, for example "255.255.255.0"
        mask: ""
        # -- A space separated list of VTEP device MAC addresses (VTEP MAC), for example "x:x:x:x:x:x  y:y:y:y:y:y:y"
        mac: ""
      # -- (string) Allows to explicitly specify the IPv4 CIDR for native routing.
      # When specified, Cilium assumes networking for this CIDR is preconfigured and
      # hands traffic destined for that range to the Linux network stack without
      # applying any SNAT.
      # Generally speaking, specifying a native routing CIDR implies that Cilium can
      # depend on the underlying networking stack to route packets to their
      # destination. To offer a concrete example, if Cilium is configured to use
      # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
      # the user must configure the routes to reach pods, either manually or by
      # setting the auto-direct-node-routes flag.
      ipv4NativeRoutingCIDR: "10.244.0.0/24"
      # -- (string) Allows to explicitly specify the IPv6 CIDR for native routing.
      # When specified, Cilium assumes networking for this CIDR is preconfigured and
      # hands traffic destined for that range to the Linux network stack without
      # applying any SNAT.
      # Generally speaking, specifying a native routing CIDR implies that Cilium can
      # depend on the underlying networking stack to route packets to their
      # destination. To offer a concrete example, if Cilium is configured to use
      # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
      # the user must configure the routes to reach pods, either manually or by
      # setting the auto-direct-node-routes flag.
      ipv6NativeRoutingCIDR: ""
      # -- cilium-monitor sidecar.
      monitor:
        # -- Enable the cilium-monitor sidecar.
        enabled: false
      # -- Configure service load balancing
      loadBalancer:
        enabled: true
        # -- standalone enables the standalone L4LB which does not connect to
        # kube-apiserver.
        # standalone: false

        # -- algorithm is the name of the load balancing algorithm for backend
        # selection e.g. random or maglev
        algorithm: maglev

        # -- mode is the operation mode of load balancing for remote backends
        # e.g. snat, dsr, hybrid
        mode: snat

        # -- acceleration is the option to accelerate service handling via XDP
        # Applicable values can be: disabled (do not use XDP), native (XDP BPF
        # program is run directly out of the networking driver's early receive
        # path), or best-effort (use native mode XDP acceleration on devices
        # that support it).
        acceleration: best-effort
        # -- dsrDispatch configures whether IP option or IPIP encapsulation is
        # used to pass a service IP and port to remote backend
        # dsrDispatch: opt

        # -- serviceTopology enables K8s Topology Aware Hints -based service
        # endpoints filtering
        # serviceTopology: false

        # -- experimental enables support for the experimental load-balancing
        # control-plane.
        experimental: false
        # -- L7 LoadBalancer
        l7:
          # -- Enable L7 service load balancing via envoy proxy.
          # The request to a k8s service, which has specific annotation e.g. service.cilium.io/lb-l7,
          # will be forwarded to the local backend proxy to be load balanced to the service endpoints.
          # Please refer to docs for supported annotations for more configuration.
          #
          # Applicable values:
          #   - envoy: Enable L7 load balancing via envoy proxy. This will automatically set enable-envoy-config as well.
          #   - disabled: Disable L7 load balancing by way of service annotation.
          backend: disabled
          # -- List of ports from service to be automatically redirected to above backend.
          # Any service exposing one of these ports will be automatically redirected.
          # Fine-grained control can be achieved by using the service annotation.
          ports: []
          # -- Default LB algorithm
          # The default LB algorithm to be used for services, which can be overridden by the
          # service annotation (e.g. service.cilium.io/lb-l7-algorithm)
          # Applicable values: round_robin, least_request, random
          algorithm: round_robin
      # -- Configure N-S k8s service loadbalancing
      nodePort:
        # -- Enable the Cilium NodePort service implementation.
        enabled: false
        # -- Port range to use for NodePort services.
        # range: "30000,32767"

        # @schema
        # type: [null, string, array]
        # @schema
        # -- List of CIDRs for choosing which IP addresses assigned to native devices are used for NodePort load-balancing.
        # By default this is empty and the first suitable, preferably private, IPv4 and IPv6 address assigned to each device is used.
        #
        # Example:
        #
        #   addresses: ["192.168.1.0/24", "2001::/64"]
        #
        addresses: ~
        # -- Set to true to prevent applications binding to service ports.
        bindProtection: true
        # -- Append NodePort range to ip_local_reserved_ports if clash with ephemeral
        # ports is detected.
        autoProtectPortRange: true
        # -- Enable healthcheck nodePort server for NodePort services
        enableHealthCheck: true
        # -- Enable access of the healthcheck nodePort on the LoadBalancerIP. Needs
        # EnableHealthCheck to be enabled
        enableHealthCheckLoadBalancerIP: false
      # policyAuditMode: false

      # -- The agent can be put into one of the three policy enforcement modes:
      # default, always and never.
      # ref: https://docs.cilium.io/en/stable/security/policy/intro/#policy-enforcement-modes
      policyEnforcementMode: "default"
      # @schema
      # type: [null, string, array]
      # @schema
      # -- policyCIDRMatchMode is a list of entities that may be selected by CIDR selector.
      # The possible value is "nodes".
      policyCIDRMatchMode:
      pprof:
        # -- Enable pprof for cilium-agent
        enabled: false
        # -- Configure pprof listen address for cilium-agent
        address: localhost
        # -- Configure pprof listen port for cilium-agent
        port: 6060
      # -- Configure prometheus metrics on the configured port at /metrics
      prometheus:
        metricsService: false
        enabled: false
        port: 9962
        serviceMonitor:
          # -- Enable service monitors.
          # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
          enabled: false
          # -- Labels to add to ServiceMonitor cilium-agent
          labels: {}
          # -- Annotations to add to ServiceMonitor cilium-agent
          annotations: {}
          # -- jobLabel to add for ServiceMonitor cilium-agent
          jobLabel: ""
          # -- Interval for scrape metrics.
          interval: "10s"
          # -- Specify the Kubernetes namespace where Prometheus expects to find
          # service monitors configured.
          # namespace: ""
          # -- Relabeling configs for the ServiceMonitor cilium-agent
          relabelings:
            - sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: node
              replacement: ${1}
          # @schema
          # type: [null, array]
          # @schema
          # -- Metrics relabeling configs for the ServiceMonitor cilium-agent
          metricRelabelings: ~
          # -- Set to `true` and helm will not check for monitoring.coreos.com/v1 CRDs before deploying
          trustCRDsExist: false
        # @schema
        # type: [null, array]
        # @schema
        # -- Metrics that should be enabled or disabled from the default metric list.
        # The list is expected to be separated by a space. (+metric_foo to enable
        # metric_foo , -metric_bar to disable metric_bar).
        # ref: https://docs.cilium.io/en/stable/observability/metrics/
        metrics: ~
        # --- Enable controller group metrics for monitoring specific Cilium
        # subsystems. The list is a list of controller group names. The special
        # values of "all" and "none" are supported. The set of controller
        # group names is not guaranteed to be stable between Cilium versions.
        controllerGroupMetrics:
          - write-cni-file
          - sync-host-ips
          - sync-lb-maps-with-k8s-services
      # -- Grafana dashboards for cilium-agent
      # grafana can import dashboards based on the label and value
      # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
      dashboards:
        enabled: false
        label: grafana_dashboard
        # @schema
        # type: [null, string]
        # @schema
        namespace: ~
        labelValue: "1"
        annotations: {}
      # Configure Cilium Envoy options.
      envoy:
        # @schema
        # type: [null, boolean]
        # @schema
        # -- Enable Envoy Proxy in standalone DaemonSet.
        # This field is enabled by default for new installation.
        # @default -- `true` for new installation
        enabled: ~
        # -- (int)
        # Set Envoy'--base-id' to use when allocating shared memory regions.
        # Only needs to be changed if multiple Envoy instances will run on the same node and may have conflicts. Supported values: 0 - 4294967295. Defaults to '0'
        baseID: 0
        log:
          # @schema
          # type: [null, string]
          # @schema
          # -- The format string to use for laying out the log message metadata of Envoy. If specified, Envoy will use text format output.
          # This setting is mutually exclusive with envoy.log.format_json.
          format: "[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v"
          # @schema
          # type: [null, object]
          # @schema
          # -- The JSON logging format to use for Envoy. This setting is mutually exclusive with envoy.log.format.
          # ref: https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/bootstrap/v3/bootstrap.proto#envoy-v3-api-field-config-bootstrap-v3-bootstrap-applicationlogconfig-logformat-json-format
          format_json: null
          # date: "%Y-%m-%dT%T.%e"
          # thread_id: "%t"
          # source_line: "%s:%#"
          # level: "%l"
          # logger: "%n"
          # message: "%j"
          # -- Path to a separate Envoy log file, if any. Defaults to /dev/stdout.
          path: ""
          # @schema
          # oneOf:
          # - type: [null]
          # - enum: [trace,debug,info,warning,error,critical,off]
          # @schema
          # -- Default log level of Envoy application log that is configured if Cilium debug / verbose logging isn't enabled.
          # This option allows to have a different log level than the Cilium Agent - e.g. lower it to `critical`.
          # Possible values: trace, debug, info, warning, error, critical, off
          # @default -- Defaults to the default log level of the Cilium Agent - `info`
          defaultLevel: ~
          # @schema
          # type: [null, integer]
          # @schema
          # -- Size of the Envoy access log buffer created within the agent in bytes.
          # Tune this value up if you encounter "Envoy: Discarded truncated access log message" errors.
          # Large request/response header sizes (e.g. 16KiB) will require a larger buffer size.
          accessLogBufferSize: 4096
        # -- Time in seconds after which a TCP connection attempt times out
        connectTimeoutSeconds: 2
        # -- Time in seconds after which the initial fetch on an xDS stream is considered timed out
        initialFetchTimeoutSeconds: 30
        # -- Maximum number of concurrent retries on Envoy clusters
        maxConcurrentRetries: 128
        # -- Maximum number of retries for each HTTP request
        httpRetryCount: 3
        # -- ProxyMaxRequestsPerConnection specifies the max_requests_per_connection setting for Envoy
        maxRequestsPerConnection: 0
        # -- Set Envoy HTTP option max_connection_duration seconds. Default 0 (disable)
        maxConnectionDurationSeconds: 0
        # -- Set Envoy upstream HTTP idle connection timeout seconds.
        # Does not apply to connections with pending requests. Default 60s
        idleTimeoutDurationSeconds: 60
        # -- Number of trusted hops regarding the x-forwarded-for and related HTTP headers for the ingress L7 policy enforcement Envoy listeners.
        xffNumTrustedHopsL7PolicyIngress: 0
        # -- Number of trusted hops regarding the x-forwarded-for and related HTTP headers for the egress L7 policy enforcement Envoy listeners.
        xffNumTrustedHopsL7PolicyEgress: 0
        # @schema
        # type: [null, string]
        # @schema
        # -- Max duration to wait for endpoint policies to be restored on restart. Default "3m".
        policyRestoreTimeoutDuration: null
        # -- Envoy container image.
        image:
          # @schema
          # type: [null, string]
          # @schema
          override: ~
          repository: "quay.io/cilium/cilium-envoy"
          tag: "v1.32.5-1744305768-f9ddca7dcd91f7ca25a505560e655c47d3dec2cf"
          pullPolicy: "IfNotPresent"
          digest: "sha256:a01cadf7974409b5c5c92ace3d6afa298408468ca24cab1cb413c04f89d3d1f9"
          useDigest: true
        # -- Additional containers added to the cilium Envoy DaemonSet.
        extraContainers: []
        # -- Additional envoy container arguments.
        extraArgs: []
        # -- Additional envoy container environment variables.
        extraEnv: []
        # -- Additional envoy hostPath mounts.
        extraHostPathMounts: []
        # - name: host-mnt-data
        #   mountPath: /host/mnt/data
        #   hostPath: /mnt/data
        #   hostPathType: Directory
        #   readOnly: true
        #   mountPropagation: HostToContainer

        # -- Additional envoy volumes.
        extraVolumes: []
        # -- Additional envoy volumeMounts.
        extraVolumeMounts: []
        # -- Configure termination grace period for cilium-envoy DaemonSet.
        terminationGracePeriodSeconds: 1
        # -- TCP port for the health API.
        healthPort: 9878
        # -- cilium-envoy update strategy
        # ref: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#updating-a-daemonset
        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            # @schema
            # type: [integer, string]
            # @schema
            maxUnavailable: 2
        # -- Roll out cilium envoy pods automatically when configmap is updated.
        rollOutPods: false
        # -- ADVANCED OPTION: Bring your own custom Envoy bootstrap ConfigMap. Provide the name of a ConfigMap with a `bootstrap-config.json` key.
        # When specified, Envoy will use this ConfigMap instead of the default provided by the chart.
        # WARNING: Use of this setting has the potential to prevent cilium-envoy from starting up, and can cause unexpected behavior (e.g. due to
        # syntax error or semantically incorrect configuration). Before submitting an issue, please ensure you have disabled this feature, as support
        # cannot be provided for custom Envoy bootstrap configs.
        # @schema
        # type: [null, string]
        # @schema
        bootstrapConfigMap: ~
        # -- Annotations to be added to all top-level cilium-envoy objects (resources under templates/cilium-envoy)
        annotations: {}
        # -- Security Context for cilium-envoy pods.
        podSecurityContext:
          # -- AppArmorProfile options for the `cilium-agent` and init containers
          appArmorProfile:
            type: "Unconfined"
        # -- Annotations to be added to envoy pods
        podAnnotations: {}
        # -- Labels to be added to envoy pods
        podLabels: {}
        # -- Envoy resource limits & requests
        # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources: {}
        #   limits:
        #     cpu: 4000m
        #     memory: 4Gi
        #   requests:
        #     cpu: 100m
        #     memory: 512Mi

        startupProbe:
          # -- failure threshold of startup probe.
          # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
          failureThreshold: 105
          # -- interval between checks of the startup probe
          periodSeconds: 2
        livenessProbe:
          # -- failure threshold of liveness probe
          failureThreshold: 10
          # -- interval between checks of the liveness probe
          periodSeconds: 30
        readinessProbe:
          # -- failure threshold of readiness probe
          failureThreshold: 3
          # -- interval between checks of the readiness probe
          periodSeconds: 30
        securityContext:
          # -- User to run the pod with
          # runAsUser: 0
          # -- Run the pod with elevated privileges
          privileged: false
          # -- SELinux options for the `cilium-envoy` container
          seLinuxOptions:
            level: 's0'
            # Running with spc_t since we have removed the privileged mode.
            # Users can change it to a different type as long as they have the
            # type available on the system.
            type: 'spc_t'
          capabilities:
            # -- Capabilities for the `cilium-envoy` container.
            # Even though granted to the container, the cilium-envoy-starter wrapper drops
            # all capabilities after forking the actual Envoy process.
            # `NET_BIND_SERVICE` is the only capability that can be passed to the Envoy process by
            # setting `envoy.securityContext.capabilities.keepNetBindService=true` (in addition to granting the
            # capability to the container).
            # Note: In case of embedded envoy, the capability must  be granted to the cilium-agent container.
            envoy:
              # Used since cilium proxy uses setting IPPROTO_IP/IP_TRANSPARENT
              - NET_ADMIN
              # We need it for now but might not need it for >= 5.11 specially
              # for the 'SYS_RESOURCE'.
              # In >= 5.8 there's already BPF and PERMON capabilities
              - SYS_ADMIN
              # Both PERFMON and BPF requires kernel 5.8, container runtime
              # cri-o >= v1.22.0 or containerd >= v1.5.0.
              # If available, SYS_ADMIN can be removed.
              #- PERFMON
              #- BPF
            # -- Keep capability `NET_BIND_SERVICE` for Envoy process.
            keepCapNetBindService: false
        # -- Affinity for cilium-envoy.
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    k8s-app: cilium-envoy
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    k8s-app: cilium
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: cilium.io/no-schedule
                      operator: NotIn
                      values:
                        - "true"
        # -- Node selector for cilium-envoy.
        nodeSelector:
          kubernetes.io/os: linux
        # -- Node tolerations for envoy scheduling to nodes with taints
        # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
        tolerations:
          - operator: Exists
            # - key: "key"
            #   operator: "Equal|Exists"
            #   value: "value"
            #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
        # @schema
        # type: [null, string]
        # @schema
        # -- The priority class to use for cilium-envoy.
        priorityClassName: ~
        # @schema
        # type: [null, string]
        # @schema
        # -- DNS policy for Cilium envoy pods.
        # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
        dnsPolicy: ~
        debug:
          admin:
            # -- Enable admin interface for cilium-envoy.
            # This is useful for debugging and should not be enabled in production.
            enabled: false
            # -- Port number (bound to loopback interface).
            # kubectl port-forward can be used to access the admin interface.
            port: 9901
        # -- Configure Cilium Envoy Prometheus options.
        # Note that some of these apply to either cilium-agent or cilium-envoy.
        prometheus:
          # -- Enable prometheus metrics for cilium-envoy
          enabled: true
          serviceMonitor:
            # -- Enable service monitors.
            # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
            # Note that this setting applies to both cilium-envoy _and_ cilium-agent
            # with Envoy enabled.
            enabled: false
            # -- Labels to add to ServiceMonitor cilium-envoy
            labels: {}
            # -- Annotations to add to ServiceMonitor cilium-envoy
            annotations: {}
            # -- Interval for scrape metrics.
            interval: "10s"
            # -- Specify the Kubernetes namespace where Prometheus expects to find
            # service monitors configured.
            # namespace: ""
            # -- Relabeling configs for the ServiceMonitor cilium-envoy
            # or for cilium-agent with Envoy configured.
            relabelings:
              - sourceLabels:
                  - __meta_kubernetes_pod_node_name
                targetLabel: node
                replacement: ${1}
            # @schema
            # type: [null, array]
            # @schema
            # -- Metrics relabeling configs for the ServiceMonitor cilium-envoy
            # or for cilium-agent with Envoy configured.
            metricRelabelings: ~
          # -- Serve prometheus metrics for cilium-envoy on the configured port
          port: "9964"
      # -- Enable/Disable use of node label based identity
      nodeSelectorLabels: false
      # -- Enable resource quotas for priority classes used in the cluster.
      resourceQuotas:
        enabled: false
        cilium:
          hard:
            # 5k nodes * 2 DaemonSets (Cilium and cilium node init)
            pods: "10k"
        operator:
          hard:
            # 15 "clusterwide" Cilium Operator pods for HA
            pods: "15"
      # Need to document default
      ##################
      #sessionAffinity: false

      # -- Do not run Cilium agent when running with clean mode. Useful to completely
      # uninstall Cilium as it will stop Cilium from starting and create artifacts
      # in the node.
      sleepAfterInit: false
      # -- Enable check of service source ranges (currently, only for LoadBalancer).
      svcSourceRangeCheck: true
      # -- Synchronize Kubernetes nodes to kvstore and perform CNP GC.
      synchronizeK8sNodes: true
      # -- Configure TLS configuration in the agent.
      tls:
        # @schema
        # type: [null, string]
        # @schema
        # -- This configures how the Cilium agent loads the secrets used TLS-aware CiliumNetworkPolicies
        # (namely the secrets referenced by terminatingTLS and originatingTLS).
        # This value is DEPRECATED and will be removed in a future version.
        # Use `tls.readSecretsOnlyFromSecretsNamespace` instead.
        # Possible values:
        #   - local
        #   - k8s
        secretsBackend: ~
        # @schema
        # type: [null, boolean]
        # @schema  
        # -- Configure if the Cilium Agent will only look in `tls.secretsNamespace` for
        #    CiliumNetworkPolicy relevant Secrets.
        #    If false, the Cilium Agent will be granted READ (GET/LIST/WATCH) access
        #    to _all_ secrets in the entire cluster. This is not recommended and is
        #    included for backwards compatibility.
        #    This value obsoletes `tls.secretsBackend`, with `true` == `local` in the old
        #    setting, and `false` == `k8s`.
        readSecretsOnlyFromSecretsNamespace: ~
        # -- Configures where secrets used in CiliumNetworkPolicies will be looked for
        secretsNamespace:
          # -- Create secrets namespace for TLS Interception secrets.
          create: true
          # -- Name of TLS Interception secret namespace.
          name: cilium-secrets
        # -- Configures settings for synchronization of TLS Interception Secrets
        secretSync:
          # @schema
          # type: [null, boolean]
          # @schema
          # -- Enable synchronization of Secrets for TLS Interception. If disabled and
          # tls.readSecretsOnlyFromSecretsNamespace is set to 'false', then secrets will be read directly by the agent.
          enabled: ~
        # -- Base64 encoded PEM values for the CA certificate and private key.
        # This can be used as common CA to generate certificates used by hubble and clustermesh components.
        # It is neither required nor used when cert-manager is used to generate the certificates.
        ca:
          # -- Optional CA cert. If it is provided, it will be used by cilium to
          # generate all other certificates. Otherwise, an ephemeral CA is generated.
          cert: ""
          # -- Optional CA private key. If it is provided, it will be used by cilium to
          # generate all other certificates. Otherwise, an ephemeral CA is generated.
          key: ""
          # -- Generated certificates validity duration in days. This will be used for auto generated CA.
          certValidityDuration: 1095
        # -- Configure the CA trust bundle used for the validation of the certificates
        # leveraged by hubble and clustermesh. When enabled, it overrides the content of the
        # 'ca.crt' field of the respective certificates, allowing for CA rotation with no down-time.
        caBundle:
          # -- Enable the use of the CA trust bundle.
          enabled: false
          # -- Name of the ConfigMap containing the CA trust bundle.
          name: cilium-root-ca.crt
          # -- Entry of the ConfigMap containing the CA trust bundle.
          key: ca.crt
          # -- Use a Secret instead of a ConfigMap.
          useSecret: false
          # If uncommented, creates the ConfigMap and fills it with the specified content.
          # Otherwise, the ConfigMap is assumed to be already present in .Release.Namespace.
          #
          # content: |
          #   -----BEGIN CERTIFICATE-----
          #   ...
          #   -----END CERTIFICATE-----
          #   -----BEGIN CERTIFICATE-----
          #   ...
          #   -----END CERTIFICATE-----
      # -- Tunneling protocol to use in tunneling mode and for ad-hoc tunnels.
      # Possible values:
      #   - ""
      #   - vxlan
      #   - geneve
      # @default -- `"vxlan"`
      tunnelProtocol: ""
      # -- Enable native-routing mode or tunneling mode.
      # Possible values:
      #   - ""
      #   - native
      #   - tunnel
      # @default -- `"tunnel"`
      routingMode: "native"
      # -- Configure VXLAN and Geneve tunnel port.
      # @default -- Port 8472 for VXLAN, Port 6081 for Geneve
      tunnelPort: 0
      # -- Configure VXLAN and Geneve tunnel source port range hint.
      # @default -- 0-0 to let the kernel driver decide the range
      tunnelSourcePortRange: 0-0
      # -- Configure what the response should be to traffic for a service without backends.
      # Possible values:
      #  - reject (default)
      #  - drop
      serviceNoBackendResponse: reject
      # -- Configure the underlying network MTU to overwrite auto-detected MTU.
      # This value doesn't change the host network interface MTU i.e. eth0 or ens0.
      # It changes the MTU for cilium_net@cilium_host, cilium_host@cilium_net,
      # cilium_vxlan and lxc_health interfaces.
      MTU: 0
      # -- Disable the usage of CiliumEndpoint CRD.
      disableEndpointCRD: false
      wellKnownIdentities:
        # -- Enable the use of well-known identities.
        enabled: false
      etcd:
        # -- Enable etcd mode for the agent.
        enabled: false
        # -- List of etcd endpoints
        endpoints:
          - https://CHANGE-ME:2379
        # -- Enable use of TLS/SSL for connectivity to etcd.
        ssl: false
      operator:
        # -- Enable the cilium-operator component (required).
        enabled: true
        # -- Roll out cilium-operator pods automatically when configmap is updated.
        rollOutPods: false
        # -- cilium-operator image.
        image:
          # @schema
          # type: [null, string]
          # @schema
          override: ~
          repository: "quay.io/cilium/operator"
          tag: "v1.18.4"
          # operator-generic-digest
          genericDigest: ""
          # operator-azure-digest
          azureDigest: ""
          # operator-aws-digest
          awsDigest: ""
          # operator-alibabacloud-digest
          alibabacloudDigest: ""
          useDigest: false
          pullPolicy: "IfNotPresent"
          suffix: ""
        # -- Number of replicas to run for the cilium-operator deployment
        replicas: 1
        # -- The priority class to use for cilium-operator
        priorityClassName: ""
        # -- DNS policy for Cilium operator pods.
        # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
        dnsPolicy: ""
        # -- cilium-operator update strategy
        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            # @schema
            # type: [integer, string]
            # @schema
            maxSurge: 25%
            # @schema
            # type: [integer, string]
            # @schema
            maxUnavailable: 50%
        # -- Affinity for cilium-operator
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    io.cilium/app: operator
        # -- Pod topology spread constraints for cilium-operator
        topologySpreadConstraints: []
        #   - maxSkew: 1
        #     topologyKey: topology.kubernetes.io/zone
        #     whenUnsatisfiable: DoNotSchedule

        # -- Node labels for cilium-operator pod assignment
        # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
        nodeSelector:
          kubernetes.io/os: linux
        # -- Node tolerations for cilium-operator scheduling to nodes with taints
        # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
        tolerations:
          - operator: Exists
            # - key: "key"
            #   operator: "Equal|Exists"
            #   value: "value"
            #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
        # -- Additional cilium-operator container arguments.
        extraArgs: []
        # -- Additional cilium-operator environment variables.
        extraEnv: []
        # -- Additional cilium-operator hostPath mounts.
        extraHostPathMounts: []
        # - name: host-mnt-data
        #   mountPath: /host/mnt/data
        #   hostPath: /mnt/data
        #   hostPathType: Directory
        #   readOnly: true
        #   mountPropagation: HostToContainer

        # -- Additional cilium-operator volumes.
        extraVolumes: []
        # -- Additional cilium-operator volumeMounts.
        extraVolumeMounts: []
        # -- Annotations to be added to all top-level cilium-operator objects (resources under templates/cilium-operator)
        annotations: {}
        # -- HostNetwork setting
        hostNetwork: true
        # -- Security context to be added to cilium-operator pods
        podSecurityContext: {}
        # -- Annotations to be added to cilium-operator pods
        podAnnotations: {}
        # -- Labels to be added to cilium-operator pods
        podLabels: {}
        # PodDisruptionBudget settings
        podDisruptionBudget:
          # -- enable PodDisruptionBudget
          # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
          enabled: false
          # @schema
          # type: [null, integer, string]
          # @schema
          # -- Minimum number/percentage of pods that should remain scheduled.
          # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
          minAvailable: null
          # @schema
          # type: [null, integer, string]
          # @schema
          # -- Maximum number/percentage of pods that may be made unavailable
          maxUnavailable: 1
        # -- cilium-operator resource limits & requests
        # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources: {}
        #   limits:
        #     cpu: 1000m
        #     memory: 1Gi
        #   requests:
        #     cpu: 100m
        #     memory: 128Mi

        # -- Security context to be added to cilium-operator pods
        securityContext: {}
        # runAsUser: 0

        # -- Interval for endpoint garbage collection.
        endpointGCInterval: "5m0s"
        # -- Interval for cilium node garbage collection.
        nodeGCInterval: "5m0s"
        # -- Interval for identity garbage collection.
        identityGCInterval: "15m0s"
        # -- Timeout for identity heartbeats.
        identityHeartbeatTimeout: "30m0s"
        pprof:
          # -- Enable pprof for cilium-operator
          enabled: false
          # -- Configure pprof listen address for cilium-operator
          address: localhost
          # -- Configure pprof listen port for cilium-operator
          port: 6061
        # -- Enable prometheus metrics for cilium-operator on the configured port at
        # /metrics
        prometheus:
          metricsService: false
          enabled: true
          port: 9963
          serviceMonitor:
            # -- Enable service monitors.
            # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
            enabled: false
            # -- Labels to add to ServiceMonitor cilium-operator
            labels: {}
            # -- Annotations to add to ServiceMonitor cilium-operator
            annotations: {}
            # -- jobLabel to add for ServiceMonitor cilium-operator
            jobLabel: ""
            # -- Interval for scrape metrics.
            interval: "10s"
            # @schema
            # type: [null, array]
            # @schema
            # -- Relabeling configs for the ServiceMonitor cilium-operator
            relabelings: ~
            # @schema
            # type: [null, array]
            # @schema
            # -- Metrics relabeling configs for the ServiceMonitor cilium-operator
            metricRelabelings: ~
        # -- Grafana dashboards for cilium-operator
        # grafana can import dashboards based on the label and value
        # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
        dashboards:
          enabled: false
          label: grafana_dashboard
          # @schema
          # type: [null, string]
          # @schema
          namespace: ~
          labelValue: "1"
          annotations: {}
        # -- Skip CRDs creation for cilium-operator
        skipCRDCreation: false
        # -- Remove Cilium node taint from Kubernetes nodes that have a healthy Cilium
        # pod running.
        removeNodeTaints: true
        # @schema
        # type: [null, boolean]
        # @schema
        # -- Taint nodes where Cilium is scheduled but not running. This prevents pods
        # from being scheduled to nodes where Cilium is not the default CNI provider.
        # @default -- same as removeNodeTaints
        setNodeTaints: ~
        # -- Set Node condition NetworkUnavailable to 'false' with the reason
        # 'CiliumIsUp' for nodes that have a healthy Cilium pod.
        setNodeNetworkStatus: true
        unmanagedPodWatcher:
          # -- Restart any pod that are not managed by Cilium.
          restart: true
          # -- Interval, in seconds, to check if there are any pods that are not
          # managed by Cilium.
          intervalSeconds: 15
      nodeinit:
        # -- Enable the node initialization DaemonSet
        enabled: false
        # -- node-init image.
        image:
          # @schema
          # type: [null, string]
          # @schema
          override: ~
          repository: "quay.io/cilium/startup-script"
          tag: "c54c7edeab7fde4da68e59acd319ab24af242c3f"
          digest: "sha256:8d7b41c4ca45860254b3c19e20210462ef89479bb6331d6760c4e609d651b29c"
          useDigest: true
          pullPolicy: "IfNotPresent"
        # -- The priority class to use for the nodeinit pod.
        priorityClassName: ""
        # -- node-init update strategy
        updateStrategy:
          type: RollingUpdate
        # -- Additional nodeinit environment variables.
        extraEnv: []
        # -- Additional nodeinit volumes.
        extraVolumes: []
        # -- Additional nodeinit volumeMounts.
        extraVolumeMounts: []
        # -- Affinity for cilium-nodeinit
        affinity: {}
        # -- Node labels for nodeinit pod assignment
        # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
        nodeSelector:
          kubernetes.io/os: linux
        # -- Node tolerations for nodeinit scheduling to nodes with taints
        # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
        tolerations:
          - operator: Exists
            # - key: "key"
            #   operator: "Equal|Exists"
            #   value: "value"
            #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
        # -- Annotations to be added to all top-level nodeinit objects (resources under templates/cilium-nodeinit)
        annotations: {}
        # -- Annotations to be added to node-init pods.
        podAnnotations: {}
        # -- Labels to be added to node-init pods.
        podLabels: {}
        # -- Security Context for cilium-node-init pods.
        podSecurityContext:
          # -- AppArmorProfile options for the `cilium-node-init` and init containers
          appArmorProfile:
            type: "Unconfined"
        # -- nodeinit resource limits & requests
        # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        # -- Security context to be added to nodeinit pods.
        securityContext:
          privileged: false
          seLinuxOptions:
            level: 's0'
            # Running with spc_t since we have removed the privileged mode.
            # Users can change it to a different type as long as they have the
            # type available on the system.
            type: 'spc_t'
          capabilities:
            add:
              # Used in iptables. Consider removing once we are iptables-free
              - SYS_MODULE
              # Used for nsenter
              - NET_ADMIN
              - SYS_ADMIN
              - SYS_CHROOT
              - SYS_PTRACE
        # -- bootstrapFile is the location of the file where the bootstrap timestamp is
        # written by the node-init DaemonSet
        bootstrapFile: "/tmp/cilium-bootstrap.d/cilium-bootstrap-time"
        # -- startup offers way to customize startup nodeinit script (pre and post position)
        startup:
          preScript: ""
          postScript: ""
        # -- prestop offers way to customize prestop nodeinit script (pre and post position)
        prestop:
          preScript: ""
          postScript: ""
      preflight:
        # -- Enable Cilium pre-flight resources (required for upgrade)
        enabled: false
        # -- Cilium pre-flight image.
        image:
          # @schema
          # type: [null, string]
          # @schema
          override: ~
          repository: "quay.io/cilium/cilium"
          tag: "v1.18.4"
          # cilium-digest
          digest: ""
          useDigest: false
          pullPolicy: "IfNotPresent"
        # -- The priority class to use for the preflight pod.
        priorityClassName: ""
        # -- preflight update strategy
        updateStrategy:
          type: RollingUpdate
        # -- Additional preflight environment variables.
        extraEnv: []
        # -- Additional preflight volumes.
        extraVolumes: []
        # -- Additional preflight volumeMounts.
        extraVolumeMounts: []
        # -- Affinity for cilium-preflight
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    k8s-app: cilium
        # -- Node labels for preflight pod assignment
        # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
        nodeSelector:
          kubernetes.io/os: linux
        # -- Node tolerations for preflight scheduling to nodes with taints
        # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
        tolerations:
          - operator: Exists
            # - key: "key"
            #   operator: "Equal|Exists"
            #   value: "value"
            #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"
        # -- Annotations to be added to all top-level preflight objects (resources under templates/cilium-preflight)
        annotations: {}
        # -- Security context to be added to preflight pods.
        podSecurityContext: {}
        # -- Annotations to be added to preflight pods
        podAnnotations: {}
        # -- Labels to be added to the preflight pod.
        podLabels: {}
        # PodDisruptionBudget settings
        podDisruptionBudget:
          # -- enable PodDisruptionBudget
          # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
          enabled: false
          # @schema
          # type: [null, integer, string]
          # @schema
          # -- Minimum number/percentage of pods that should remain scheduled.
          # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
          minAvailable: null
          # @schema
          # type: [null, integer, string]
          # @schema
          # -- Maximum number/percentage of pods that may be made unavailable
          maxUnavailable: 1
        # -- preflight resource limits & requests
        # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources: {}
        #   limits:
        #     cpu: 4000m
        #     memory: 4Gi
        #   requests:
        #     cpu: 100m
        #     memory: 512Mi

        readinessProbe:
          # -- For how long kubelet should wait before performing the first probe
          initialDelaySeconds: 5
          # -- interval between checks of the readiness probe
          periodSeconds: 5
        # -- Security context to be added to preflight pods
        securityContext: {}
        #   runAsUser: 0

        # -- Path to write the `--tofqdns-pre-cache` file to.
        tofqdnsPreCache: ""
        # -- Configure termination grace period for preflight Deployment and DaemonSet.
        terminationGracePeriodSeconds: 1
        # -- By default we should always validate the installed CNPs before upgrading
        # Cilium. This will make sure the user will have the policies deployed in the
        # cluster with the right schema.
        validateCNPs: true
      # -- Explicitly enable or disable priority class.
      # .Capabilities.KubeVersion is unsettable in `helm template` calls,
      # it depends on k8s libraries version that Helm was compiled against.
      # This option allows to explicitly disable setting the priority class, which
      # is useful for rendering charts for gke clusters in advance.
      enableCriticalPriorityClass: true
      # disableEnvoyVersionCheck removes the check for Envoy, which can be useful
      # on AArch64 as the images do not currently ship a version of Envoy.
      #disableEnvoyVersionCheck: false
      clustermesh:
        # -- Deploy clustermesh-apiserver for clustermesh
        useAPIServer: false
        # -- The maximum number of clusters to support in a ClusterMesh. This value
        # cannot be changed on running clusters, and all clusters in a ClusterMesh
        # must be configured with the same value. Values > 255 will decrease the
        # maximum allocatable cluster-local identities.
        # Supported values are 255 and 511.
        maxConnectedClusters: 255
        # -- Enable the synchronization of Kubernetes EndpointSlices corresponding to
        # the remote endpoints of appropriately-annotated global services through ClusterMesh
        enableEndpointSliceSynchronization: false
        # -- Enable Multi-Cluster Services API support
        enableMCSAPISupport: false
        # -- Annotations to be added to all top-level clustermesh objects (resources under templates/clustermesh-apiserver and templates/clustermesh-config)
        annotations: {}
        # -- Clustermesh explicit configuration.
        config:
          # -- Enable the Clustermesh explicit configuration.
          enabled: false
          # -- Default dns domain for the Clustermesh API servers
          # This is used in the case cluster addresses are not provided
          # and IPs are used.
          domain: mesh.cilium.io
          # -- List of clusters to be peered in the mesh.
          clusters: []
          # clusters:
          # # -- Name of the cluster
          # - name: cluster1
          # # -- Address of the cluster, use this if you created DNS records for
          # # the cluster Clustermesh API server.
          #   address: cluster1.mesh.cilium.io
          # # -- Port of the cluster Clustermesh API server.
          #   port: 2379
          # # -- IPs of the cluster Clustermesh API server, use multiple ones when
          # # you have multiple IPs to access the Clustermesh API server.
          #   ips:
          #   - 172.18.255.201
          # # -- base64 encoded PEM values for the cluster client certificate, private key and certificate authority.
          # # These fields can (and should) be omitted in case the CA is shared across clusters. In that case, the
          # # "remote" private key and certificate available in the local cluster are automatically used instead.
          #   tls:
          #     cert: ""
          #     key: ""
          #     caCert: ""
        apiserver:
          # -- Clustermesh API server image.
          image:
            # @schema
            # type: [null, string]
            # @schema
            override: ~
            repository: "quay.io/cilium/clustermesh-apiserver"
            tag: "v1.18.4"
            # clustermesh-apiserver-digest
            digest: ""
            useDigest: false
            pullPolicy: "IfNotPresent"
          # -- TCP port for the clustermesh-apiserver health API.
          healthPort: 9880
          # -- Configuration for the clustermesh-apiserver readiness probe.
          readinessProbe: {}
          etcd:
            # The etcd binary is included in the clustermesh API server image, so the same image from above is reused.
            # Independent override isn't supported, because clustermesh-apiserver is tested against the etcd version it is
            # built with.

            # -- Specifies the resources for etcd container in the apiserver
            resources: {}
            #   requests:
            #     cpu: 200m
            #     memory: 256Mi
            #   limits:
            #     cpu: 1000m
            #     memory: 256Mi

            # -- Security context to be added to clustermesh-apiserver etcd containers
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL
            # -- lifecycle setting for the etcd container
            lifecycle: {}
            init:
              # -- Specifies the resources for etcd init container in the apiserver
              resources: {}
              #   requests:
              #     cpu: 100m
              #     memory: 100Mi
              #   limits:
              #     cpu: 100m
              #     memory: 100Mi

              # -- Additional arguments to `clustermesh-apiserver etcdinit`.
              extraArgs: []
              # -- Additional environment variables to `clustermesh-apiserver etcdinit`.
              extraEnv: []
            # @schema
            # enum: [Disk, Memory]
            # @schema
            # -- Specifies whether etcd data is stored in a temporary volume backed by
            # the node's default medium, such as disk, SSD or network storage (Disk), or
            # RAM (Memory). The Memory option enables improved etcd read and write
            # performance at the cost of additional memory usage, which counts against
            # the memory limits of the container.
            storageMedium: Disk
          kvstoremesh:
            # -- Enable KVStoreMesh. KVStoreMesh caches the information retrieved
            # from the remote clusters in the local etcd instance.
            enabled: true
            # -- TCP port for the KVStoreMesh health API.
            healthPort: 9881
            # -- Configuration for the KVStoreMesh readiness probe.
            readinessProbe: {}
            # -- Additional KVStoreMesh arguments.
            extraArgs: []
            # -- Additional KVStoreMesh environment variables.
            extraEnv: []
            # -- Resource requests and limits for the KVStoreMesh container
            resources: {}
            #   requests:
            #     cpu: 100m
            #     memory: 64Mi
            #   limits:
            #     cpu: 1000m
            #     memory: 1024M

            # -- Additional KVStoreMesh volumeMounts.
            extraVolumeMounts: []
            # -- KVStoreMesh Security context
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL
            # -- lifecycle setting for the KVStoreMesh container
            lifecycle: {}
          service:
            # -- The type of service used for apiserver access.
            type: NodePort
            # -- Optional port to use as the node port for apiserver access.
            #
            # WARNING: make sure to configure a different NodePort in each cluster if
            # kube-proxy replacement is enabled, as Cilium is currently affected by a known
            # bug (#24692) when NodePorts are handled by the KPR implementation. If a service
            # with the same NodePort exists both in the local and the remote cluster, all
            # traffic originating from inside the cluster and targeting the corresponding
            # NodePort will be redirected to a local backend, regardless of whether the
            # destination node belongs to the local or the remote cluster.
            nodePort: 32379
            # -- Annotations for the clustermesh-apiserver service.
            # Example annotations to configure an internal load balancer on different cloud providers:
            # * AKS: service.beta.kubernetes.io/azure-load-balancer-internal: "true"
            # * EKS: service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
            # * GKE: networking.gke.io/load-balancer-type: "Internal"
            annotations: {}
            # @schema
            # enum: [Local, Cluster]
            # @schema
            # -- The externalTrafficPolicy of service used for apiserver access.
            externalTrafficPolicy: Cluster
            # @schema
            # enum: [Local, Cluster]
            # @schema
            # -- The internalTrafficPolicy of service used for apiserver access.
            internalTrafficPolicy: Cluster
            # @schema
            # enum: [HAOnly, Always, Never]
            # @schema
            # -- Defines when to enable session affinity.
            # Each replica in a clustermesh-apiserver deployment runs its own discrete
            # etcd cluster. Remote clients connect to one of the replicas through a
            # shared Kubernetes Service. A client reconnecting to a different backend
            # will require a full resync to ensure data integrity. Session affinity
            # can reduce the likelihood of this happening, but may not be supported
            # by all cloud providers.
            # Possible values:
            #  - "HAOnly" (default) Only enable session affinity for deployments with more than 1 replica.
            #  - "Always" Always enable session affinity.
            #  - "Never" Never enable session affinity. Useful in environments where
            #            session affinity is not supported, but may lead to slightly
            #            degraded performance due to more frequent reconnections.
            enableSessionAffinity: "HAOnly"
            # @schema
            # type: [null, string]
            # @schema
            # -- Configure a loadBalancerClass.
            # Allows to configure the loadBalancerClass on the clustermesh-apiserver
            # LB service in case the Service type is set to LoadBalancer
            # (requires Kubernetes 1.24+).
            loadBalancerClass: ~
            # @schema
            # type: [null, string]
            # @schema
            # -- Configure a specific loadBalancerIP.
            # Allows to configure a specific loadBalancerIP on the clustermesh-apiserver
            # LB service in case the Service type is set to LoadBalancer.
            loadBalancerIP: ~
            # -- Configure loadBalancerSourceRanges.
            # Allows to configure the source IP ranges allowed to access the
            # clustermesh-apiserver LB service in case the Service type is set to LoadBalancer.
            loadBalancerSourceRanges: []
          # -- Number of replicas run for the clustermesh-apiserver deployment.
          replicas: 1
          # -- lifecycle setting for the apiserver container
          lifecycle: {}
          # -- terminationGracePeriodSeconds for the clustermesh-apiserver deployment
          terminationGracePeriodSeconds: 30
          # -- Additional clustermesh-apiserver arguments.
          extraArgs: []
          # -- Additional clustermesh-apiserver environment variables.
          extraEnv: []
          # -- Additional clustermesh-apiserver volumes.
          extraVolumes: []
          # -- Additional clustermesh-apiserver volumeMounts.
          extraVolumeMounts: []
          # -- Security context to be added to clustermesh-apiserver containers
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          # -- Security context to be added to clustermesh-apiserver pods
          podSecurityContext:
            runAsNonRoot: true
            runAsUser: 65532
            runAsGroup: 65532
            fsGroup: 65532
          # -- Annotations to be added to clustermesh-apiserver pods
          podAnnotations: {}
          # -- Labels to be added to clustermesh-apiserver pods
          podLabels: {}
          # PodDisruptionBudget settings
          podDisruptionBudget:
            # -- enable PodDisruptionBudget
            # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
            enabled: false
            # @schema
            # type: [null, integer, string]
            # @schema
            # -- Minimum number/percentage of pods that should remain scheduled.
            # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
            minAvailable: null
            # @schema
            # type: [null, integer, string]
            # @schema
            # -- Maximum number/percentage of pods that may be made unavailable
            maxUnavailable: 1
          # -- Resource requests and limits for the clustermesh-apiserver
          resources: {}
          #   requests:
          #     cpu: 100m
          #     memory: 64Mi
          #   limits:
          #     cpu: 1000m
          #     memory: 1024M

          # -- Affinity for clustermesh.apiserver
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchLabels:
                        k8s-app: clustermesh-apiserver
                    topologyKey: kubernetes.io/hostname
          # -- Pod topology spread constraints for clustermesh-apiserver
          topologySpreadConstraints: []
          #   - maxSkew: 1
          #     topologyKey: topology.kubernetes.io/zone
          #     whenUnsatisfiable: DoNotSchedule

          # -- Node labels for pod assignment
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
          nodeSelector:
            kubernetes.io/os: linux
          # -- Node tolerations for pod assignment on nodes with taints
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
          tolerations: []
          # -- clustermesh-apiserver update strategy
          updateStrategy:
            type: RollingUpdate
            rollingUpdate:
              # @schema
              # type: [integer, string]
              # @schema
              maxSurge: 1
              # @schema
              # type: [integer, string]
              # @schema
              maxUnavailable: 0
          # -- The priority class to use for clustermesh-apiserver
          priorityClassName: ""
          tls:
            # -- Configure the clustermesh authentication mode.
            # Supported values:
            # - legacy:     All clusters access remote clustermesh instances with the same
            #               username (i.e., remote). The "remote" certificate must be
            #               generated with CN=remote if provided manually.
            # - migration:  Intermediate mode required to upgrade from legacy to cluster
            #               (and vice versa) with no disruption. Specifically, it enables
            #               the creation of the per-cluster usernames, while still using
            #               the common one for authentication. The "remote" certificate must
            #               be generated with CN=remote if provided manually (same as legacy).
            # - cluster:    Each cluster accesses remote etcd instances with a username
            #               depending on the local cluster name (i.e., remote-<cluster-name>).
            #               The "remote" certificate must be generated with CN=remote-<cluster-name>
            #               if provided manually. Cluster mode is meaningful only when the same
            #               CA is shared across all clusters part of the mesh.
            authMode: legacy
            # -- Allow users to provide their own certificates
            # Users may need to provide their certificates using
            # a mechanism that requires they provide their own secrets.
            # This setting does not apply to any of the auto-generated
            # mechanisms below, it only restricts the creation of secrets
            # via the `tls-provided` templates.
            enableSecrets: true
            # -- Configure automatic TLS certificates generation.
            # A Kubernetes CronJob is used the generate any
            # certificates not provided by the user at installation
            # time.
            auto:
              # -- When set to true, automatically generate a CA and certificates to
              # enable mTLS between clustermesh-apiserver and external workload instances.
              # If set to false, the certs to be provided by setting appropriate values below.
              enabled: true
              # Sets the method to auto-generate certificates. Supported values:
              # - helm:         This method uses Helm to generate all certificates.
              # - cronJob:      This method uses a Kubernetes CronJob the generate any
              #                 certificates not provided by the user at installation
              #                 time.
              # - certmanager:  This method use cert-manager to generate & rotate certificates.
              method: helm
              # -- Generated certificates validity duration in days.
              certValidityDuration: 1095
              # -- Schedule for certificates regeneration (regardless of their expiration date).
              # Only used if method is "cronJob". If nil, then no recurring job will be created.
              # Instead, only the one-shot job is deployed to generate the certificates at
              # installation time.
              #
              # Due to the out-of-band distribution of client certs to external workloads the
              # CA is (re)regenerated only if it is not provided as a helm value and the k8s
              # secret is manually deleted.
              #
              # Defaults to none. Commented syntax gives midnight of the first day of every
              # fourth month. For syntax, see
              # https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax
              # schedule: "0 0 1 */4 *"

              # [Example]
              # certManagerIssuerRef:
              #   group: cert-manager.io
              #   kind: ClusterIssuer
              #   name: ca-issuer
              # -- certmanager issuer used when clustermesh.apiserver.tls.auto.method=certmanager.
              certManagerIssuerRef: {}
            # -- base64 encoded PEM values for the clustermesh-apiserver server certificate and private key.
            # Used if 'auto' is not enabled.
            server:
              cert: ""
              key: ""
              # -- Extra DNS names added to certificate when it's auto generated
              extraDnsNames: []
              # -- Extra IP addresses added to certificate when it's auto generated
              extraIpAddresses: []
            # -- base64 encoded PEM values for the clustermesh-apiserver admin certificate and private key.
            # Used if 'auto' is not enabled.
            admin:
              cert: ""
              key: ""
            # -- base64 encoded PEM values for the clustermesh-apiserver client certificate and private key.
            # Used if 'auto' is not enabled.
            client:
              cert: ""
              key: ""
            # -- base64 encoded PEM values for the clustermesh-apiserver remote cluster certificate and private key.
            # Used if 'auto' is not enabled.
            remote:
              cert: ""
              key: ""
          # clustermesh-apiserver Prometheus metrics configuration
          metrics:
            # -- Enables exporting apiserver metrics in OpenMetrics format.
            enabled: true
            # -- Configure the port the apiserver metric server listens on.
            port: 9962
            kvstoremesh:
              # -- Enables exporting KVStoreMesh metrics in OpenMetrics format.
              enabled: true
              # -- Configure the port the KVStoreMesh metric server listens on.
              port: 9964
            etcd:
              # -- Enables exporting etcd metrics in OpenMetrics format.
              enabled: true
              # -- Set level of detail for etcd metrics; specify 'extensive' to include server side gRPC histogram metrics.
              mode: basic
              # -- Configure the port the etcd metric server listens on.
              port: 9963
            serviceMonitor:
              # -- Enable service monitor.
              # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
              enabled: false
              # -- Labels to add to ServiceMonitor clustermesh-apiserver
              labels: {}
              # -- Annotations to add to ServiceMonitor clustermesh-apiserver
              annotations: {}
              # -- Specify the Kubernetes namespace where Prometheus expects to find
              # service monitors configured.
              # namespace: ""

              # -- Interval for scrape metrics (apiserver metrics)
              interval: "10s"
              # @schema
              # type: [null, array]
              # @schema
              # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
              relabelings: ~
              # @schema
              # type: [null, array]
              # @schema
              # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
              metricRelabelings: ~
              kvstoremesh:
                # -- Interval for scrape metrics (KVStoreMesh metrics)
                interval: "10s"
                # @schema
                # type: [null, array]
                # @schema
                # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
                relabelings: ~
                # @schema
                # type: [null, array]
                # @schema
                # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
                metricRelabelings: ~
              etcd:
                # -- Interval for scrape metrics (etcd metrics)
                interval: "10s"
                # @schema
                # type: [null, array]
                # @schema
                # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
                relabelings: ~
                # @schema
                # type: [null, array]
                # @schema
                # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
                metricRelabelings: ~
      # -- Configure external workloads support
      externalWorkloads:
        # -- Enable support for external workloads, such as VMs (false by default).
        enabled: false
      # -- Configure cgroup related configuration
      cgroup:
        autoMount:
          # -- Enable auto mount of cgroup2 filesystem.
          # When `autoMount` is enabled, cgroup2 filesystem is mounted at
          # `cgroup.hostRoot` path on the underlying host and inside the cilium agent pod.
          # If users disable `autoMount`, it's expected that users have mounted
          # cgroup2 filesystem at the specified `cgroup.hostRoot` volume, and then the
          # volume will be mounted inside the cilium agent pod at the same path.
          enabled: false
          # -- Init Container Cgroup Automount resource limits & requests
          resources: {}
          #   limits:
          #     cpu: 100m
          #     memory: 128Mi
          #   requests:
          #     cpu: 100m
          #     memory: 128Mi
        # -- Configure cgroup root where cgroup2 filesystem is mounted on the host (see also: `cgroup.autoMount`)
        hostRoot: /sys/fs/cgroup
      # -- Configure sysctl override described in #20072.
      sysctlfix:
        # -- Enable the sysctl override. When enabled, the init container will mount the /proc of the host so that the `sysctlfix` utility can execute.
        enabled: true
      # -- Configure whether to enable auto detect of terminating state for endpoints
      # in order to support graceful termination.
      enableK8sTerminatingEndpoint: true
      # -- Configure whether to unload DNS policy rules on graceful shutdown
      # dnsPolicyUnloadOnShutdown: false

      # -- Configure the key of the taint indicating that Cilium is not ready on the node.
      # When set to a value starting with `ignore-taint.cluster-autoscaler.kubernetes.io/`, the Cluster Autoscaler will ignore the taint on its decisions, allowing the cluster to scale up.
      agentNotReadyTaintKey: "node.cilium.io/agent-not-ready"
      dnsProxy:
        # -- Timeout (in seconds) when closing the connection between the DNS proxy and the upstream server. If set to 0, the connection is closed immediately (with TCP RST). If set to -1, the connection is closed asynchronously in the background.
        socketLingerTimeout: 10
        # -- DNS response code for rejecting DNS requests, available options are '[nameError refused]'.
        dnsRejectResponseCode: refused
        # -- Allow the DNS proxy to compress responses to endpoints that are larger than 512 Bytes or the EDNS0 option, if present.
        enableDnsCompression: true
        # -- Maximum number of IPs to maintain per FQDN name for each endpoint.
        endpointMaxIpPerHostname: 1000
        # -- Time during which idle but previously active connections with expired DNS lookups are still considered alive.
        idleConnectionGracePeriod: 0s
        # -- Maximum number of IPs to retain for expired DNS lookups with still-active connections.
        maxDeferredConnectionDeletes: 10000
        # -- The minimum time, in seconds, to use DNS data for toFQDNs policies. If
        # the upstream DNS server returns a DNS record with a shorter TTL, Cilium
        # overwrites the TTL with this value. Setting this value to zero means that
        # Cilium will honor the TTLs returned by the upstream DNS server.
        minTtl: 0
        # -- DNS cache data at this path is preloaded on agent startup.
        preCache: ""
        # -- Global port on which the in-agent DNS proxy should listen. Default 0 is a OS-assigned port.
        proxyPort: 0
        # -- The maximum time the DNS proxy holds an allowed DNS response before sending it along. Responses are sent as soon as the datapath is updated with the new IP information.
        proxyResponseMaxDelay: 100ms
        # -- DNS proxy operation mode (true/false, or unset to use version dependent defaults)
        # enableTransparentMode: true
      # -- SCTP Configuration Values
      sctp:
        # -- Enable SCTP support. NOTE: Currently, SCTP support does not support rewriting ports or multihoming.
        enabled: false
      # -- Enable Non-Default-Deny policies
      enableNonDefaultDenyPolicies: true
      # Configuration for types of authentication for Cilium (beta)
      authentication:
        # -- Enable authentication processing and garbage collection.
        # Note that if disabled, policy enforcement will still block requests that require authentication.
        # But the resulting authentication requests for these requests will not be processed, therefore the requests not be allowed.
        enabled: true
        # -- Buffer size of the channel Cilium uses to receive authentication events from the signal map.
        queueSize: 1024
        # -- Buffer size of the channel Cilium uses to receive certificate expiration events from auth handlers.
        rotatedIdentitiesQueueSize: 1024
        # -- Interval for garbage collection of auth map entries.
        gcInterval: "5m0s"
        # Configuration for Cilium's service-to-service mutual authentication using TLS handshakes.
        # Note that this is not full mTLS support without also enabling encryption of some form.
        # Current encryption options are WireGuard or IPsec, configured in encryption block above.
        mutual:
          # -- Port on the agent where mutual authentication handshakes between agents will be performed
          port: 4250
          # -- Timeout for connecting to the remote node TCP socket
          connectTimeout: 5s
          # Settings for SPIRE
          spire:
            # -- Enable SPIRE integration (beta)
            enabled: true
            # -- Annotations to be added to all top-level spire objects (resources under templates/spire)
            annotations: {}
            # Settings to control the SPIRE installation and configuration
            install:
              # -- Enable SPIRE installation.
              # This will only take effect only if authentication.mutual.spire.enabled is true
              enabled: true
              # -- SPIRE namespace to install into
              namespace: kube-service
              # -- SPIRE namespace already exists. Set to true if Helm should not create, manage, and import the SPIRE namespace.
              existingNamespace: true
              # -- init container image of SPIRE agent and server
              initImage:
                # @schema
                # type: [null, string]
                # @schema
                override: ~
                repository: "docker.io/library/busybox"
                tag: "1.37.0"
                digest: "sha256:37f7b378a29ceb4c551b1b5582e27747b855bbfaa73fa11914fe0df028dc581f"
                useDigest: true
                pullPolicy: "IfNotPresent"
              # SPIRE agent configuration
              agent:
                # -- The priority class to use for the spire agent
                priorityClassName: ""
                # -- SPIRE agent image
                image:
                  # @schema
                  # type: [null, string]
                  # @schema
                  override: ~
                  repository: "ghcr.io/spiffe/spire-agent"
                  tag: "1.9.6"
                  digest: "sha256:5106ac601272a88684db14daf7f54b9a45f31f77bb16a906bd5e87756ee7b97c"
                  useDigest: true
                  pullPolicy: "IfNotPresent"
                # -- SPIRE agent service account
                serviceAccount:
                  create: true
                  name: spire-agent
                # -- SPIRE agent annotations
                annotations: {}
                # -- SPIRE agent labels
                labels: {}
                # -- container resource limits & requests
                resources: {}
                # -- SPIRE Workload Attestor kubelet verification.
                skipKubeletVerification: true
                # -- SPIRE agent tolerations configuration
                # By default it follows the same tolerations as the agent itself
                # to allow the Cilium agent on this node to connect to SPIRE.
                # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
                tolerations:
                  - key: node.kubernetes.io/not-ready
                    effect: NoSchedule
                  - key: node-role.kubernetes.io/master
                    effect: NoSchedule
                  - key: node-role.kubernetes.io/control-plane
                    effect: NoSchedule
                  - key: node.cloudprovider.kubernetes.io/uninitialized
                    effect: NoSchedule
                    value: "true"
                  - key: CriticalAddonsOnly
                    operator: "Exists"
                # -- SPIRE agent affinity configuration
                affinity: {}
                # -- SPIRE agent nodeSelector configuration
                # ref: ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
                nodeSelector: {}
                # -- Security context to be added to spire agent pods.
                # SecurityContext holds pod-level security attributes and common container settings.
                # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
                podSecurityContext:
                  fsGroup: 1000
                  runAsUser: 1000
                  runAsGroup: 1000
                # -- Security context to be added to spire agent containers.
                # SecurityContext holds pod-level security attributes and common container settings.
                # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                      - ALL
              server:
                # -- The priority class to use for the spire server
                priorityClassName: ""
                # -- SPIRE server image
                image:
                  override: ~
                  repository: "ghcr.io/spiffe/spire-server"
                  tag: "1.9.6"
                  digest: "sha256:59a0b92b39773515e25e68a46c40d3b931b9c1860bc445a79ceb45a805cab8b4"
                  useDigest: true
                  pullPolicy: "IfNotPresent"
                serviceAccount:
                  create: true
                  name: spire-server
                initContainers: []
                annotations: {}
                labels: {}
                resources: {}
                service:
                  type: ClusterIP
                  annotations: {}
                  labels: {}
                affinity: {}
                nodeSelector: {}
                tolerations: []
                dataStorage:
                  enabled: true
                  size: 1Gi
                  accessMode: ReadWriteOnce
                  storageClass: null
                podSecurityContext:
                  fsGroup: 1000
                  runAsUser: 1000
                  runAsGroup: 1000
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                      - ALL
                ca:
                  keyType: "rsa-4096"
                  subject:
                    country: "US"
                    organization: "SPIRE"
                    commonName: "Cilium SPIRE CA"
            serverAddress: ~
            trustDomain: spiffe.cilium
            adminSocketPath: /run/spire/sockets/admin.sock
            agentSocketPath: /run/spire/sockets/agent/agent.sock
            connectionTimeout: 30s
      enableInternalTrafficPolicy: true
      enableLBIPAM: true
