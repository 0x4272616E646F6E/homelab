apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-gpt-oss
  namespace: development
  labels:
    app.kubernetes.io/name: llamacpp-gpt-oss
    app.kubernetes.io/managed-by: flux
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp-gpt-oss
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxSurge: 0, maxUnavailable: 1 }
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp-gpt-oss
        app.kubernetes.io/managed-by: flux
    spec:
      runtimeClassName: nvidia
      terminationGracePeriodSeconds: 300
      automountServiceAccountToken: false
      containers:
      - name: llamacpp-gpt-oss
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:718d3bd1ba1ee1cecf66d70b516b4d6c9d8d5d59de85eca45e522fbf7858d8b5
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - mradermacher/gpt-oss-20b-heretic-v2-GGUF
          - --hf-file
          - gpt-oss-20b-heretic-v2.MXFP4_MOE.gguf
          - --host
          - "0.0.0.0"
          - --port
          - "8000"
          - --ctx-size
          - "24576"
          - --n-gpu-layers
          - "99"
          - --parallel
          - "2"
          - --batch-size
          - "1024"
          - --ubatch-size
          - "512"
          - --flash-attn
          - auto
          - --cache-type-k
          - q8_0
          - --cache-type-v
          - q8_0
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8000, name: http }
        resources:
          requests: { cpu: "2", memory: "12Gi", nvidia.com/gpu: "1", ephemeral-storage: "10Gi" }
          limits:   { cpu: "8", memory: "16Gi", nvidia.com/gpu: "1", ephemeral-storage: "30Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /models }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: http }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: http }
          periodSeconds: 15
          failureThreshold: 60
      volumes:
      - name: cache-volume
        persistentVolumeClaim: { claimName: gpt-oss-pvc }
      - name: tmp
        emptyDir: {}
