apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-model
  namespace: development
  labels:
    app.kubernetes.io/name: llamacpp-model
    app.kubernetes.io/managed-by: flux
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp-model
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxSurge: 0, maxUnavailable: 1 }
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp-model
        app.kubernetes.io/managed-by: flux
    spec:
      runtimeClassName: nvidia
      terminationGracePeriodSeconds: 300
      automountServiceAccountToken: false
      containers:
      - name: llamacpp-model
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:ebb819ea14432bc92a78faeadac2bcd8c8e9a604b0405b146b12c9622472e5b5
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - MuXodious/GLM-4.7-Flash-REAP-23B-A3B-absolute-heresy-GGUF
          - --hf-file
          - GLM-4.7-Flash-REAP-23B-A3B-absolute-heresy-Q4_K_M.gguf
          - --alias
          - GLM-4.7-Flash-REAP
          - --host
          - "0.0.0.0"
          - --port
          - "8000"
          - --n-gpu-layers
          - "999"
          - --ctx-size
          - "131072"
          - --cache-type-k
          - q8_0
          - --cache-type-v
          - q8_0
          - --parallel
          - "1"
          - --flash-attn
          - "auto"
          - --temp
          - "0.7"
          - --top-p
          - "1.0"
          - --jinja
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8000, name: http }
        resources:
          requests: { cpu: "2", memory: "16Gi", nvidia.com/gpu: "1", ephemeral-storage: "10Gi" }
          limits:   { cpu: "8", memory: "32Gi", nvidia.com/gpu: "1", ephemeral-storage: "30Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /root/.cache/llama.cpp }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: http }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: http }
          periodSeconds: 15
          failureThreshold: 60
      volumes:
      - name: cache-volume
        persistentVolumeClaim: { claimName: llamacpp-model-pvc }
      - name: tmp
        emptyDir: {}
