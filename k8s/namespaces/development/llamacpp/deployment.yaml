apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-model
  namespace: development
  labels:
    app.kubernetes.io/name: llamacpp-model
    app.kubernetes.io/managed-by: flux
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp-model
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxSurge: 0, maxUnavailable: 1 }
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp-model
        app.kubernetes.io/managed-by: flux
    spec:
      runtimeClassName: nvidia
      terminationGracePeriodSeconds: 300
      automountServiceAccountToken: false
      containers:
      - name: llamacpp-model
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:8903d304f9cadf35ac881ebf0bb3537426b5b096b63088d0f17b719656b07c20
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - unsloth/Qwen3.5-27B-GGUF
          - --hf-file
          - Qwen3.5-27B-UD-Q4_K_XL.gguf
          - --alias
          - qwen-3.5-27b
          - --host
          - "0.0.0.0"
          - --port
          - "8000"
          - --ctx-size
          - "32768"
          - --n-gpu-layers
          - "99"
          - --batch-size
          - "512"
          - --ubatch-size
          - "128"
          - --flash-attn
          - auto
          - --cache-type-k
          - q8_0
          - --cache-type-v
          - q8_0
          - --temp 
          - "0.6"
          - --top-p 
          - "0.95"
          - --top-k 
          - "20"
          - --min-p 
          - "0.00"
          - --jinja 
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8000, name: http }
        resources:
          requests: { cpu: "2", memory: "12Gi", nvidia.com/gpu: "1", ephemeral-storage: "10Gi" }
          limits:   { cpu: "8", memory: "18Gi", nvidia.com/gpu: "1", ephemeral-storage: "30Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /root/.cache/llama.cpp }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: http }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: http }
          periodSeconds: 15
          failureThreshold: 60
      volumes:
      - name: cache-volume
        persistentVolumeClaim: { claimName: llamacpp-model-pvc }
      - name: tmp
        emptyDir: {}
