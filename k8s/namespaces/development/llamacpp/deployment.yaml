apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-model
  namespace: development
  labels:
    app.kubernetes.io/name: llamacpp-model
    app.kubernetes.io/managed-by: flux
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp-model
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxSurge: 0, maxUnavailable: 1 }
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp-model
        app.kubernetes.io/managed-by: flux
    spec:
      runtimeClassName: nvidia
      terminationGracePeriodSeconds: 300
      automountServiceAccountToken: false
      containers:
      - name: llamacpp-model
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:047fcd1b4a1d3861517699dedb6cd69e7936623bd272cb571917c05e1eee12c2
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - mradermacher/Llama-3.3-8B-Instruct-heretic-i1-GGUF
          - --hf-file
          - Llama-3.3-8B-Instruct-heretic.i1-Q4_K_M.gguf
          - --alias
          - llama-3.3-8b
          - --host
          - "0.0.0.0"
          - --port
          - "8000"
          - --ctx-size
          - "131072"
          - --n-gpu-layers
          - "99"
          - --tensor-split
          - "0"
          - --split-mode
          - layer
          - --main-gpu
          - "0"
          - --parallel
          - "1"
          - --batch-size
          - "256"
          - --ubatch-size
          - "128"
          - --flash-attn
          - auto
          - --cache-type-k
          - q8_0
          - --cache-type-v
          - q8_0
          - --temp
          - "0.3"
          - --top-p
          - "0.9"
          - --top-k
          - "40"
          - --min-p
          - "0.05"
          - --jinja 
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8000, name: http }
        resources:
          requests: { cpu: "2", memory: "16Gi", nvidia.com/gpu: "1", ephemeral-storage: "10Gi" }
          limits:   { cpu: "8", memory: "24Gi", nvidia.com/gpu: "1", ephemeral-storage: "30Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /root/.cache/llama.cpp }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: http }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: http }
          periodSeconds: 15
          failureThreshold: 60
      - name: llamacpp-embeddings
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:047fcd1b4a1d3861517699dedb6cd69e7936623bd272cb571917c05e1eee12c2
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - jinaai/jina-embeddings-v4-text-retrieval-GGUF
          - --hf-file
          - jina-embeddings-v4-text-retrieval-Q4_K_M.gguf
          - --alias
          - jina-embeddings-v4
          - --host
          - "0.0.0.0"
          - --port
          - "8001"
          - --ctx-size
          - "8192"
          - --n-gpu-layers
          - "99"
          - --embedding
          - --pooling
          - mean
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8001, name: embeddings }
        resources:
          requests: { cpu: "1", memory: "2Gi", ephemeral-storage: "5Gi" }
          limits:   { cpu: "2", memory: "4Gi", ephemeral-storage: "10Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /root/.cache/llama.cpp }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: embeddings }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: embeddings }
          periodSeconds: 15
          failureThreshold: 60
      - name: llamacpp-reranker
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:047fcd1b4a1d3861517699dedb6cd69e7936623bd272cb571917c05e1eee12c2
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - jinaai/jina-reranker-v3-GGUF
          - --hf-file
          - jina-reranker-v3-Q4_K_M.gguf
          - --alias
          - jina-reranker-v3
          - --host
          - "0.0.0.0"
          - --port
          - "8002"
          - --ctx-size
          - "8192"
          - --n-gpu-layers
          - "99"
          - --reranking
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8002, name: reranker }
        resources:
          requests: { cpu: "1", memory: "2Gi", ephemeral-storage: "5Gi" }
          limits:   { cpu: "2", memory: "4Gi", ephemeral-storage: "10Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /root/.cache/llama.cpp }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: reranker }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: reranker }
          periodSeconds: 15
          failureThreshold: 60
      volumes:
      - name: cache-volume
        persistentVolumeClaim: { claimName: llamacpp-model-pvc }
      - name: tmp
        emptyDir: {}
