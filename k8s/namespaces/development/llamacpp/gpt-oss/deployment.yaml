apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp-gpt-oss
  namespace: development
  labels:
    app.kubernetes.io/name: llamacpp-gpt-oss
    app.kubernetes.io/managed-by: flux
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llamacpp-gpt-oss
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxSurge: 0, maxUnavailable: 1 }
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llamacpp-gpt-oss
        app.kubernetes.io/managed-by: flux
    spec:
      runtimeClassName: nvidia
      terminationGracePeriodSeconds: 300
      automountServiceAccountToken: false
      containers:
      - name: llamacpp-gpt-oss
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:306f9d8d9d42951762c5a4e8ef7d6e033e2537c291cd1075eae503fc9a82bdcc
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - mradermacher/gpt-oss-20b-heretic-v2-GGUF
          - --hf-file
          - gpt-oss-20b-heretic-v2.MXFP4_MOE.gguf
          - --alias
          - gpt-oss-20b
          - --host
          - "0.0.0.0"
          - --port
          - "8000"
          - --ctx-size
          - "128000"
          - --n-gpu-layers
          - "99"
          - --tensor-split
          - "0"
          - --split-mode
          - layer
          - --main-gpu
          - "0"
          - --parallel
          - "1"
          - --batch-size
          - "512"
          - --ubatch-size
          - "256"
          - --flash-attn
          - auto
          - --cache-type-k
          - q8_0
          - --cache-type-v
          - q8_0
          - --temp
          - "1.0"
          - --top-p
          - "1.0"
          - --top-k
          - "40"
          - --min-p
          - "0.001"
          - --jinja 
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8000, name: http }
        resources:
          requests: { cpu: "2", memory: "24Gi", nvidia.com/gpu: "1", ephemeral-storage: "10Gi" }
          limits:   { cpu: "8", memory: "32Gi", nvidia.com/gpu: "1", ephemeral-storage: "30Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /root/.cache/llama.cpp }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: http }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: http }
          periodSeconds: 15
          failureThreshold: 60
      - name: llamacpp-embeddings
        # renovate: datasource=docker depName=ghcr.io/ggml-org/llama.cpp
        image: ghcr.io/ggml-org/llama.cpp:server-cuda@sha256:306f9d8d9d42951762c5a4e8ef7d6e033e2537c291cd1075eae503fc9a82bdcc
        imagePullPolicy: IfNotPresent
        args:
          - --hf-repo
          - gaianet/jina-embeddings-v3-GGUF
          - --hf-file
          - jina-embeddings-v3-Q8_0.gguf
          - --alias
          - jina-embeddings-v3
          - --host
          - "0.0.0.0"
          - --port
          - "8001"
          - --ctx-size
          - "8192"
          - --n-gpu-layers
          - "99"
          - --embedding
          - --pooling
          - mean
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom: { secretKeyRef: { name: vllm-secrets, key: token } }
        - { name: HF_HOME,               value: /models/.cache/hf }
        - { name: HUGGINGFACE_HUB_CACHE, value: /models/.cache/huggingface }
        - { name: CUDA_VISIBLE_DEVICES,  value: "0" }
        ports:
        - { containerPort: 8001, name: embeddings }
        resources:
          requests: { cpu: "1", memory: "2Gi", ephemeral-storage: "5Gi" }
          limits:   { cpu: "2", memory: "4Gi", ephemeral-storage: "10Gi" }
        securityContext:
          runAsUser: 0
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities: { drop: ["ALL"] }
        volumeMounts:
        - { name: cache-volume, mountPath: /root/.cache/llama.cpp }
        - { name: tmp,          mountPath: /tmp }
        readinessProbe:
          httpGet: { path: /health, port: embeddings }
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30
        startupProbe:
          httpGet: { path: /health, port: embeddings }
          periodSeconds: 15
          failureThreshold: 60
      volumes:
      - name: cache-volume
        persistentVolumeClaim: { claimName: gpt-oss-pvc }
      - name: tmp
        emptyDir: {}
